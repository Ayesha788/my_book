<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-chapter4-vla" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 4: Vision-Language-Action (VLA) - Converting Voice Commands to Robot Actions | Physical AI &amp; Humanoid Robotics Hackathon Book</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ayesha788.github.io/my_book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ayesha788.github.io/my_book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ayesha788.github.io/my_book/docs/chapter4-vla"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 4: Vision-Language-Action (VLA) - Converting Voice Commands to Robot Actions | Physical AI &amp; Humanoid Robotics Hackathon Book"><meta data-rh="true" name="description" content="Introduction to Vision-Language-Action Systems"><meta data-rh="true" property="og:description" content="Introduction to Vision-Language-Action Systems"><link data-rh="true" rel="icon" href="/my_book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ayesha788.github.io/my_book/docs/chapter4-vla"><link data-rh="true" rel="alternate" href="https://ayesha788.github.io/my_book/docs/chapter4-vla" hreflang="en"><link data-rh="true" rel="alternate" href="https://ayesha788.github.io/my_book/docs/chapter4-vla" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Chapter 4: Vision-Language-Action (VLA) - Converting Voice Commands to Robot Actions","item":"https://ayesha788.github.io/my_book/docs/chapter4-vla"}]}</script><link rel="stylesheet" href="/my_book/assets/css/styles.26d1bab7.css">
<script src="/my_book/assets/js/runtime~main.cc4ca827.js" defer="defer"></script>
<script src="/my_book/assets/js/main.3daa9b1f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/my_book/"><div class="navbar__logo"><img src="/my_book/img/logo.svg" alt="Physical AI &amp; Robotics Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/my_book/img/logo.svg" alt="Physical AI &amp; Robotics Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Robotics Book</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/my_book/docs/intro">Book Chapters</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Ayesha788/my_book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/my_book/docs/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/my_book/docs/environment-setup"><span title="Environment Setup" class="linkLabel_WmDU">Environment Setup</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter1-ros2"><span title="ROS 2 Fundamentals" class="categoryLinkLabel_W154">ROS 2 Fundamentals</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter2-digital-twin"><span title="Digital Twin &amp; Simulation" class="categoryLinkLabel_W154">Digital Twin &amp; Simulation</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter3-isaac"><span title="AI-Robot Brain" class="categoryLinkLabel_W154">AI-Robot Brain</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/my_book/docs/chapter4-vla"><span title="Vision-Language-Action" class="categoryLinkLabel_W154">Vision-Language-Action</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/my_book/docs/chapter4-vla"><span title="Chapter 4: Vision-Language-Action (VLA) - Converting Voice Commands to Robot Actions" class="linkLabel_WmDU">Chapter 4: Vision-Language-Action (VLA) - Converting Voice Commands to Robot Actions</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter5-path-planning"><span title="Path Planning &amp; Navigation" class="categoryLinkLabel_W154">Path Planning &amp; Navigation</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter6-computer-vision"><span title="Computer Vision" class="categoryLinkLabel_W154">Computer Vision</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter7-manipulation"><span title="Manipulation &amp; Grasping" class="categoryLinkLabel_W154">Manipulation &amp; Grasping</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter8-multirobot"><span title="Multi-Robot Coordination" class="categoryLinkLabel_W154">Multi-Robot Coordination</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter9-learning"><span title="Learning &amp; Adaptation" class="categoryLinkLabel_W154">Learning &amp; Adaptation</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter10-hri"><span title="Human-Robot Interaction" class="categoryLinkLabel_W154">Human-Robot Interaction</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter11-safety"><span title="Safety &amp; Ethics" class="categoryLinkLabel_W154">Safety &amp; Ethics</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/chapter12-future"><span title="Future Trends &amp; Applications" class="categoryLinkLabel_W154">Future Trends &amp; Applications</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/practice-tasks-ros2"><span title="Practice Tasks" class="categoryLinkLabel_W154">Practice Tasks</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/my_book/docs/glossary"><span title="Reference" class="categoryLinkLabel_W154">Reference</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/my_book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language-Action</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Chapter 4: Vision-Language-Action (VLA) - Converting Voice Commands to Robot Actions</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1 id="chapter-4-vision-language-action-vla---converting-voice-commands-to-robot-actions">Chapter 4: Vision-Language-Action (VLA) - Converting Voice Commands to Robot Actions</h1></header>
<h2 id="introduction-to-vision-language-action-systems">Introduction to Vision-Language-Action Systems</h2>
<p>Vision-Language-Action (VLA) systems represent the cutting edge of human-robot interaction, enabling robots to understand natural language commands, perceive their environment visually, and execute appropriate physical actions. This convergence of computer vision, natural language processing, and robotics creates intuitive interfaces that allow humans to interact with robots using everyday language.</p>
<p>For humanoid robots, VLA systems enable:</p>
<ul>
<li>Natural voice command interpretation</li>
<li>Visual scene understanding</li>
<li>Cognitive planning and task execution</li>
<li>Adaptive behavior based on environmental context</li>
</ul>
<h2 id="architecture-of-vla-systems">Architecture of VLA Systems</h2>
<h3 id="core-components">Core Components</h3>
<p>A typical VLA system consists of:</p>
<ol>
<li><strong>Speech Recognition</strong>: Converting voice commands to text</li>
<li><strong>Natural Language Understanding</strong>: Interpreting the meaning of commands</li>
<li><strong>Visual Perception</strong>: Understanding the current environment</li>
<li><strong>Cognitive Planning</strong>: Determining appropriate actions</li>
<li><strong>Action Execution</strong>: Sending commands to robot actuators</li>
</ol>
<h3 id="integration-with-ros-2">Integration with ROS 2</h3>
<p>VLA systems integrate with ROS 2 through:</p>
<ul>
<li>Message passing for sensor data and commands</li>
<li>Action servers for long-running tasks</li>
<li>Services for immediate queries</li>
<li>Parameter servers for configuration</li>
</ul>
<h2 id="voice-command-processing-with-openai-whisper">Voice Command Processing with OpenAI Whisper</h2>
<p>OpenAI Whisper is a state-of-the-art speech recognition model that can transcribe speech to text with high accuracy across multiple languages.</p>
<h3 id="installing-whisper">Installing Whisper</h3>
<pre><code class="language-bash">pip install openai-whisper
# Or for GPU acceleration
pip install openai-whisper[cuda]
</code></pre>
<h3 id="basic-whisper-integration">Basic Whisper Integration</h3>
<pre><code class="language-python">import whisper
import rospy
import pyaudio
import wave
import numpy as np
from std_msgs.msg import String

class VoiceCommandProcessor:
    def __init__(self):
        rospy.init_node(&#x27;voice_command_processor&#x27;)

        # Load Whisper model
        self.model = whisper.load_model(&quot;base&quot;)  # Options: tiny, base, small, medium, large

        # Audio parameters
        self.chunk = 1024
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = 44100
        self.record_seconds = 5

        # Publishers and subscribers
        self.command_pub = rospy.Publisher(&#x27;/vla/voice_command&#x27;, String, queue_size=10)
        self.result_pub = rospy.Publisher(&#x27;/vla/command_result&#x27;, String, queue_size=10)

        # Initialize audio stream
        self.audio = pyaudio.PyAudio()

        rospy.loginfo(&quot;Voice Command Processor initialized&quot;)

    def record_audio(self):
        &quot;&quot;&quot;Record audio from microphone&quot;&quot;&quot;
        stream = self.audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )

        rospy.loginfo(&quot;Recording...&quot;)
        frames = []

        for i in range(0, int(self.rate / self.chunk * self.record_seconds)):
            data = stream.read(self.chunk)
            frames.append(data)

        rospy.loginfo(&quot;Recording finished&quot;)

        stream.stop_stream()
        stream.close()

        return frames

    def transcribe_audio(self, frames):
        &quot;&quot;&quot;Transcribe recorded audio using Whisper&quot;&quot;&quot;
        # Save frames to temporary WAV file
        filename = &quot;temp_recording.wav&quot;
        wf = wave.open(filename, &#x27;wb&#x27;)
        wf.setnchannels(self.channels)
        wf.setsampwidth(self.audio.get_sample_size(self.format))
        wf.setframerate(self.rate)
        wf.writeframes(b&#x27;&#x27;.join(frames))
        wf.close()

        # Transcribe using Whisper
        result = self.model.transcribe(filename)
        transcription = result[&quot;text&quot;]

        # Clean up temporary file
        import os
        os.remove(filename)

        return transcription.strip()

    def process_voice_command(self):
        &quot;&quot;&quot;Main loop for processing voice commands&quot;&quot;&quot;
        rate = rospy.Rate(1)  # Process commands once per second

        while not rospy.is_shutdown():
            try:
                # Record audio
                frames = self.record_audio()

                # Transcribe to text
                command_text = self.transcribe_audio(frames)

                if command_text:
                    rospy.loginfo(f&quot;Recognized command: {command_text}&quot;)

                    # Publish the recognized command
                    cmd_msg = String()
                    cmd_msg.data = command_text
                    self.command_pub.publish(cmd_msg)

                    # Process the command and execute action
                    self.execute_command(command_text)

                rate.sleep()

            except Exception as e:
                rospy.logerr(f&quot;Error processing voice command: {str(e)}&quot;)

    def execute_command(self, command_text):
        &quot;&quot;&quot;Parse command and execute appropriate action&quot;&quot;&quot;
        command_text = command_text.lower()

        if &quot;move forward&quot; in command_text:
            self.move_robot(&quot;forward&quot;)
        elif &quot;turn left&quot; in command_text:
            self.move_robot(&quot;left&quot;)
        elif &quot;turn right&quot; in command_text:
            self.move_robot(&quot;right&quot;)
        elif &quot;stop&quot; in command_text:
            self.stop_robot()
        elif &quot;pick up&quot; in command_text or &quot;grasp&quot; in command_text:
            self.grasp_object()
        elif &quot;wave&quot; in command_text:
            self.wave_gesture()
        else:
            rospy.logwarn(f&quot;Unknown command: {command_text}&quot;)
            result_msg = String()
            result_msg.data = f&quot;Unknown command: {command_text}&quot;
            self.result_pub.publish(result_msg)

    def move_robot(self, direction):
        &quot;&quot;&quot;Execute movement command&quot;&quot;&quot;
        rospy.loginfo(f&quot;Moving robot {direction}&quot;)
        # Publish movement command to robot controller
        pass

    def stop_robot(self):
        &quot;&quot;&quot;Stop robot movement&quot;&quot;&quot;
        rospy.loginfo(&quot;Stopping robot&quot;)
        # Publish stop command
        pass

    def grasp_object(self):
        &quot;&quot;&quot;Execute grasping action&quot;&quot;&quot;
        rospy.loginfo(&quot;Attempting to grasp object&quot;)
        # Publish grasping command
        pass

    def wave_gesture(self):
        &quot;&quot;&quot;Execute waving gesture&quot;&quot;&quot;
        rospy.loginfo(&quot;Waving gesture&quot;)
        # Publish waving motion command
        pass

def main():
    processor = VoiceCommandProcessor()
    try:
        processor.process_voice_command()
    except rospy.ROSInterruptException:
        pass

if __name__ == &#x27;__main__&#x27;:
    main()
</code></pre>
<h2 id="natural-language-to-ros-2-action-mapping">Natural Language to ROS 2 Action Mapping</h2>
<p>Converting natural language commands to specific ROS 2 actions requires a cognitive planning system that can interpret intent and map it to executable robot behaviors.</p>
<h3 id="intent-recognition-and-action-mapping">Intent Recognition and Action Mapping</h3>
<pre><code class="language-python">import re
import rospy
from std_msgs.msg import String
from geometry_msgs.msg import Twist
from actionlib_msgs.msg import GoalStatusArray

class CognitivePlanner:
    def __init__(self):
        rospy.init_node(&#x27;cognitive_planner&#x27;)

        # Publishers for different robot actions
        self.cmd_vel_pub = rospy.Publisher(&#x27;/cmd_vel&#x27;, Twist, queue_size=10)
        self.action_pub = rospy.Publisher(&#x27;/robot_action&#x27;, String, queue_size=10)

        # Subscriber for voice commands
        self.voice_sub = rospy.Subscriber(&#x27;/vla/voice_command&#x27;, String, self.command_callback)

        # Define command patterns and corresponding actions
        self.command_patterns = {
            r&#x27;move forward|go forward|forward&#x27;: self.move_forward,
            r&#x27;move backward|go backward|backward&#x27;: self.move_backward,
            r&#x27;turn left|rotate left|left&#x27;: self.turn_left,
            r&#x27;turn right|rotate right|right&#x27;: self.turn_right,
            r&#x27;stop|halt|freeze&#x27;: self.stop_movement,
            r&#x27;go to (.+)|move to (.+)&#x27;: self.navigate_to_location,
            r&#x27;pick up (.+)|grasp (.+)|get (.+)&#x27;: self.grasp_object,
            r&#x27;wave|waving|hello&#x27;: self.wave_gesture,
            r&#x27;follow me|follow&#x27;: self.follow_person,
        }

        rospy.loginfo(&quot;Cognitive Planner initialized&quot;)

    def command_callback(self, msg):
        &quot;&quot;&quot;Process incoming voice commands&quot;&quot;&quot;
        command = msg.data.lower().strip()
        rospy.loginfo(f&quot;Processing command: {command}&quot;)

        # Match command to pattern and execute action
        action_executed = False
        for pattern, action_func in self.command_patterns.items():
            match = re.search(pattern, command)
            if match:
                if match.groups():  # If pattern has capture groups
                    action_func(*match.groups())
                else:
                    action_func()
                action_executed = True
                break

        if not action_executed:
            rospy.logwarn(f&quot;No action matched for command: {command}&quot;)
            self.unknown_command(command)

    def move_forward(self):
        &quot;&quot;&quot;Move robot forward&quot;&quot;&quot;
        twist = Twist()
        twist.linear.x = 0.5  # Adjust speed as needed
        self.cmd_vel_pub.publish(twist)
        rospy.loginfo(&quot;Moving forward&quot;)

    def move_backward(self):
        &quot;&quot;&quot;Move robot backward&quot;&quot;&quot;
        twist = Twist()
        twist.linear.x = -0.5
        self.cmd_vel_pub.publish(twist)
        rospy.loginfo(&quot;Moving backward&quot;)

    def turn_left(self):
        &quot;&quot;&quot;Turn robot left&quot;&quot;&quot;
        twist = Twist()
        twist.angular.z = 0.5
        self.cmd_vel_pub.publish(twist)
        rospy.loginfo(&quot;Turning left&quot;)

    def turn_right(self):
        &quot;&quot;&quot;Turn robot right&quot;&quot;&quot;
        twist = Twist()
        twist.angular.z = -0.5
        self.cmd_vel_pub.publish(twist)
        rospy.loginfo(&quot;Turning right&quot;)

    def stop_movement(self):
        &quot;&quot;&quot;Stop all robot movement&quot;&quot;&quot;
        twist = Twist()
        self.cmd_vel_pub.publish(twist)
        rospy.loginfo(&quot;Stopping movement&quot;)

    def navigate_to_location(self, location):
        &quot;&quot;&quot;Navigate to specified location&quot;&quot;&quot;
        rospy.loginfo(f&quot;Navigating to {location}&quot;)
        # This would integrate with navigation stack
        # For now, just publish the location as an action
        action_msg = String()
        action_msg.data = f&quot;navigate_to:{location}&quot;
        self.action_pub.publish(action_msg)

    def grasp_object(self, object_name):
        &quot;&quot;&quot;Grasp specified object&quot;&quot;&quot;
        rospy.loginfo(f&quot;Attempting to grasp {object_name}&quot;)
        # Publish grasp command with object name
        action_msg = String()
        action_msg.data = f&quot;grasp_object:{object_name}&quot;
        self.action_pub.publish(action_msg)

    def wave_gesture(self):
        &quot;&quot;&quot;Perform waving gesture&quot;&quot;&quot;
        rospy.loginfo(&quot;Performing waving gesture&quot;)
        action_msg = String()
        action_msg.data = &quot;wave_gesture&quot;
        self.action_pub.publish(action_msg)

    def follow_person(self):
        &quot;&quot;&quot;Start following person&quot;&quot;&quot;
        rospy.loginfo(&quot;Starting person following&quot;)
        action_msg = String()
        action_msg.data = &quot;follow_person&quot;
        self.action_pub.publish(action_msg)

    def unknown_command(self, command):
        &quot;&quot;&quot;Handle unknown commands&quot;&quot;&quot;
        rospy.logwarn(f&quot;Unknown command: {command}&quot;)
        # Could implement learning or clarification here

def main():
    planner = CognitivePlanner()
    rospy.spin()

if __name__ == &#x27;__main__&#x27;:
    main()
</code></pre>
<h2 id="visual-scene-understanding">Visual Scene Understanding</h2>
<p>For effective VLA systems, robots must understand their visual environment to contextualize voice commands.</p>
<h3 id="object-detection-and-recognition">Object Detection and Recognition</h3>
<pre><code class="language-python">import cv2
import numpy as np
import rospy
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose

class VisualPerception:
    def __init__(self):
        rospy.init_node(&#x27;visual_perception&#x27;)

        # Initialize CV bridge
        self.bridge = CvBridge()

        # Subscribe to camera feed
        self.image_sub = rospy.Subscriber(&#x27;/camera/rgb/image_raw&#x27;, Image, self.image_callback)

        # Publish object detections
        self.detection_pub = rospy.Publisher(&#x27;/vla/detections&#x27;, Detection2DArray, queue_size=10)

        # Load pre-trained object detection model (e.g., YOLO)
        # For this example, we&#x27;ll use a placeholder
        self.detector = self.load_detector()

        rospy.loginfo(&quot;Visual Perception node initialized&quot;)

    def load_detector(self):
        &quot;&quot;&quot;Load object detection model&quot;&quot;&quot;
        # In practice, this would load a model like YOLOv5, Detectron2, etc.
        # For this example, we&#x27;ll return a placeholder
        return None

    def image_callback(self, msg):
        &quot;&quot;&quot;Process incoming camera images&quot;&quot;&quot;
        try:
            # Convert ROS image message to OpenCV image
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=&#x27;bgr8&#x27;)

            # Perform object detection
            detections = self.detect_objects(cv_image)

            # Publish detections
            self.publish_detections(detections, msg.header)

        except Exception as e:
            rospy.logerr(f&quot;Error processing image: {str(e)}&quot;)

    def detect_objects(self, image):
        &quot;&quot;&quot;Detect objects in the image&quot;&quot;&quot;
        # Placeholder for object detection
        # In practice, this would run inference on a deep learning model
        height, width = image.shape[:2]

        # Example detections (in practice, these would come from the model)
        detections = [
            {
                &#x27;class&#x27;: &#x27;person&#x27;,
                &#x27;confidence&#x27;: 0.95,
                &#x27;bbox&#x27;: [int(width * 0.3), int(height * 0.4), int(width * 0.2), int(height * 0.4)]
            },
            {
                &#x27;class&#x27;: &#x27;chair&#x27;,
                &#x27;confidence&#x27;: 0.87,
                &#x27;bbox&#x27;: [int(width * 0.6), int(height * 0.5), int(width * 0.2), int(height * 0.3)]
            }
        ]

        return detections

    def publish_detections(self, detections, header):
        &quot;&quot;&quot;Publish object detections as ROS messages&quot;&quot;&quot;
        detection_array = Detection2DArray()
        detection_array.header = header

        for det in detections:
            detection = Detection2D()
            detection.header = header

            # Set bounding box
            detection.bbox.size_x = det[&#x27;bbox&#x27;][2]
            detection.bbox.size_y = det[&#x27;bbox&#x27;][3]
            detection.bbox.center.x = det[&#x27;bbox&#x27;][0] + det[&#x27;bbox&#x27;][2] / 2
            detection.bbox.center.y = det[&#x27;bbox&#x27;][1] + det[&#x27;bbox&#x27;][3] / 2

            # Set classification
            hypothesis = ObjectHypothesisWithPose()
            hypothesis.id = det[&#x27;class&#x27;]
            hypothesis.score = det[&#x27;confidence&#x27;]
            detection.results.append(hypothesis)

            detection_array.detections.append(detection)

        self.detection_pub.publish(detection_array)

def main():
    perception = VisualPerception()
    rospy.spin()

if __name__ == &#x27;__main__&#x27;:
    main()
</code></pre>
<h2 id="voice-command-pipeline-integration">Voice Command Pipeline Integration</h2>
<p>Combining all components into a complete VLA system:</p>
<pre><code class="language-python">import rospy
import threading
from std_msgs.msg import String
from geometry_msgs.msg import Twist

class VLASystem:
    def __init__(self):
        rospy.init_node(&#x27;vla_system&#x27;)

        # Publishers
        self.cmd_vel_pub = rospy.Publisher(&#x27;/cmd_vel&#x27;, Twist, queue_size=10)
        self.status_pub = rospy.Publisher(&#x27;/vla/status&#x27;, String, queue_size=10)

        # Subscribers
        self.voice_sub = rospy.Subscriber(&#x27;/vla/voice_command&#x27;, String, self.voice_command_callback)
        self.vision_sub = rospy.Subscriber(&#x27;/vla/detections&#x27;, String, self.vision_callback)

        # System state
        self.current_context = {}
        self.is_listening = True

        rospy.loginfo(&quot;VLA System initialized&quot;)

    def voice_command_callback(self, msg):
        &quot;&quot;&quot;Process voice command in context of visual information&quot;&quot;&quot;
        command = msg.data

        # Combine voice command with visual context
        action = self.interpret_command_with_context(command, self.current_context)

        if action:
            self.execute_action(action)
            self.update_status(f&quot;Executed: {action}&quot;)

    def vision_callback(self, msg):
        &quot;&quot;&quot;Update visual context&quot;&quot;&quot;
        # Update current visual context based on detections
        self.current_context[&#x27;objects&#x27;] = msg.data  # Simplified
        rospy.loginfo(f&quot;Updated visual context: {msg.data}&quot;)

    def interpret_command_with_context(self, command, context):
        &quot;&quot;&quot;Interpret command using visual context&quot;&quot;&quot;
        command_lower = command.lower()

        # Example: &quot;Pick up the red ball&quot; - need to find red ball in context
        if &quot;pick up&quot; in command_lower or &quot;grasp&quot; in command_lower:
            # Look for objects that match the description in visual context
            if &quot;red ball&quot; in command_lower and &quot;red ball&quot; in context.get(&#x27;objects&#x27;, &#x27;&#x27;):
                return &quot;grasp_red_ball&quot;

        elif &quot;go to&quot; in command_lower:
            # Use navigation based on visual landmarks
            return f&quot;navigate_to:{command_lower.replace(&#x27;go to&#x27;, &#x27;&#x27;).strip()}&quot;

        # Default interpretation
        return command

    def execute_action(self, action):
        &quot;&quot;&quot;Execute the interpreted action&quot;&quot;&quot;
        rospy.loginfo(f&quot;Executing action: {action}&quot;)

        # Map action to ROS command
        if &quot;forward&quot; in action:
            self.move_forward()
        elif &quot;grasp&quot; in action:
            self.execute_grasp()
        elif &quot;navigate&quot; in action:
            self.start_navigation(action)
        # Add more action mappings as needed

    def move_forward(self):
        &quot;&quot;&quot;Move robot forward&quot;&quot;&quot;
        twist = Twist()
        twist.linear.x = 0.3
        self.cmd_vel_pub.publish(twist)

    def execute_grasp(self):
        &quot;&quot;&quot;Execute grasping action (placeholder)&quot;&quot;&quot;
        rospy.loginfo(&quot;Executing grasp action&quot;)
        # Publish grasp command to manipulation system

    def start_navigation(self, action):
        &quot;&quot;&quot;Start navigation to specified location&quot;&quot;&quot;
        rospy.loginfo(f&quot;Starting navigation: {action}&quot;)
        # Integrate with navigation stack

    def update_status(self, status):
        &quot;&quot;&quot;Publish system status&quot;&quot;&quot;
        status_msg = String()
        status_msg.data = status
        self.status_pub.publish(status_msg)

def main():
    vla_system = VLASystem()

    # Keep the node running
    try:
        rospy.spin()
    except KeyboardInterrupt:
        rospy.loginfo(&quot;VLA System shutting down&quot;)

if __name__ == &#x27;__main__&#x27;:
    main()
</code></pre>
<h2 id="integration-with-humanoid-robot-control">Integration with Humanoid Robot Control</h2>
<p>Connecting VLA systems to humanoid robot actuators requires careful consideration of the robot&#x27;s kinematic structure and safety constraints.</p>
<h3 id="humanoid-action-execution">Humanoid Action Execution</h3>
<pre><code class="language-python">import rospy
from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint
from control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryGoal
import actionlib

class HumanoidActionExecutor:
    def __init__(self):
        rospy.init_node(&#x27;humanoid_action_executor&#x27;)

        # Joint trajectory publisher for simple movements
        self.joint_pub = rospy.Publisher(&#x27;/joint_trajectory_controller/command&#x27;, JointTrajectory, queue_size=10)

        # Action client for complex movements
        self.trajectory_client = actionlib.SimpleActionClient(
            &#x27;/joint_trajectory_controller/follow_joint_trajectory&#x27;,
            FollowJointTrajectoryAction
        )

        # Wait for action server
        rospy.loginfo(&quot;Waiting for joint trajectory controller...&quot;)
        self.trajectory_client.wait_for_server()
        rospy.loginfo(&quot;Connected to joint trajectory controller&quot;)

        # Define joint names for humanoid robot
        self.joint_names = [
            &#x27;left_hip_joint&#x27;, &#x27;left_knee_joint&#x27;, &#x27;left_ankle_joint&#x27;,
            &#x27;right_hip_joint&#x27;, &#x27;right_knee_joint&#x27;, &#x27;right_ankle_joint&#x27;,
            &#x27;left_shoulder_joint&#x27;, &#x27;left_elbow_joint&#x27;, &#x27;left_wrist_joint&#x27;,
            &#x27;right_shoulder_joint&#x27;, &#x27;right_elbow_joint&#x27;, &#x27;right_wrist_joint&#x27;
        ]

    def execute_waving_motion(self):
        &quot;&quot;&quot;Execute waving gesture with arm joints&quot;&quot;&quot;
        trajectory = JointTrajectory()
        trajectory.joint_names = self.joint_names

        # Create trajectory points for waving motion
        point1 = JointTrajectoryPoint()
        point1.positions = [0.0] * len(self.joint_names)  # Default positions
        point1.positions[7] = 1.0  # Left elbow up
        point1.time_from_start = rospy.Duration(1.0)

        point2 = JointTrajectoryPoint()
        point2.positions = point1.positions[:]
        point2.positions[7] = -1.0  # Left elbow down
        point2.time_from_start = rospy.Duration(2.0)

        trajectory.points = [point1, point2]
        trajectory.header.stamp = rospy.Time.now()

        self.joint_pub.publish(trajectory)

    def execute_walking_gait(self):
        &quot;&quot;&quot;Execute basic walking gait&quot;&quot;&quot;
        trajectory = JointTrajectory()
        trajectory.joint_names = self.joint_names

        # Simplified walking gait (in practice, this would be much more complex)
        points = []
        time_step = 0.5

        for i in range(10):  # 10 steps
            point = JointTrajectoryPoint()
            point.positions = [0.0] * len(self.joint_names)

            # Alternate leg movements for walking
            if i % 2 == 0:
                point.positions[0] = 0.2  # Left hip forward
                point.positions[3] = -0.1  # Right hip back
            else:
                point.positions[0] = -0.1  # Left hip back
                point.positions[3] = 0.2  # Right hip forward

            point.time_from_start = rospy.Duration(i * time_step)
            points.append(point)

        trajectory.points = points
        trajectory.header.stamp = rospy.Time.now()

        self.joint_pub.publish(trajectory)

    def execute_voice_command(self, command):
        &quot;&quot;&quot;Map voice command to humanoid action&quot;&quot;&quot;
        command_lower = command.lower()

        if &quot;wave&quot; in command_lower:
            self.execute_waving_motion()
        elif &quot;walk&quot; in command_lower or &quot;move&quot; in command_lower:
            self.execute_walking_gait()
        elif &quot;stand&quot; in command_lower:
            self.stand_up()
        # Add more commands as needed

    def stand_up(self):
        &quot;&quot;&quot;Return to standing position&quot;&quot;&quot;
        trajectory = JointTrajectory()
        trajectory.joint_names = self.joint_names

        point = JointTrajectoryPoint()
        point.positions = [0.0] * len(self.joint_names)  # Neutral standing position
        point.time_from_start = rospy.Duration(2.0)

        trajectory.points = [point]
        trajectory.header.stamp = rospy.Time.now()

        self.joint_pub.publish(trajectory)

def main():
    executor = HumanoidActionExecutor()

    # Example: Execute waving motion when node starts
    rospy.sleep(1.0)  # Wait for publishers to connect
    executor.execute_waving_motion()

    rospy.spin()

if __name__ == &#x27;__main__&#x27;:
    main()
</code></pre>
<h2 id="practice-tasks">Practice Tasks</h2>
<ol>
<li>Install and configure OpenAI Whisper for voice recognition</li>
<li>Create a simple voice command that makes your robot move forward</li>
<li>Implement object detection that identifies basic objects in the camera feed</li>
<li>Develop a system that combines voice commands with visual information (e.g., &quot;pick up the red ball&quot;)</li>
<li>Create a gesture that your humanoid robot performs when it hears &quot;hello&quot;</li>
</ol>
<h2 id="summary">Summary</h2>
<p>In this chapter, you&#x27;ve learned to build Vision-Language-Action systems that enable natural human-robot interaction:</p>
<ul>
<li>How to integrate OpenAI Whisper for voice command recognition</li>
<li>Techniques for mapping natural language to ROS 2 actions</li>
<li>Methods for incorporating visual scene understanding</li>
<li>Approaches for combining voice, vision, and action in a unified system</li>
<li>Implementation of humanoid-specific action execution</li>
</ul>
<p>VLA systems represent the future of human-robot interaction, making robots more accessible and intuitive to use. By combining speech recognition, visual perception, and intelligent action planning, you can create humanoid robots that respond naturally to human commands and adapt to their environment.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/Ayesha788/my_book/tree/main/docs/chapter4-vla.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/my_book/docs/chapter3-isaac"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Chapter 3: AI-Robot Brain - NVIDIA Isaac Integration</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/my_book/docs/chapter5-path-planning"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Chapter 5: Path Planning and Navigation for Humanoid Robots</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction-to-vision-language-action-systems" class="table-of-contents__link toc-highlight">Introduction to Vision-Language-Action Systems</a></li><li><a href="#architecture-of-vla-systems" class="table-of-contents__link toc-highlight">Architecture of VLA Systems</a><ul><li><a href="#core-components" class="table-of-contents__link toc-highlight">Core Components</a></li><li><a href="#integration-with-ros-2" class="table-of-contents__link toc-highlight">Integration with ROS 2</a></li></ul></li><li><a href="#voice-command-processing-with-openai-whisper" class="table-of-contents__link toc-highlight">Voice Command Processing with OpenAI Whisper</a><ul><li><a href="#installing-whisper" class="table-of-contents__link toc-highlight">Installing Whisper</a></li><li><a href="#basic-whisper-integration" class="table-of-contents__link toc-highlight">Basic Whisper Integration</a></li></ul></li><li><a href="#natural-language-to-ros-2-action-mapping" class="table-of-contents__link toc-highlight">Natural Language to ROS 2 Action Mapping</a><ul><li><a href="#intent-recognition-and-action-mapping" class="table-of-contents__link toc-highlight">Intent Recognition and Action Mapping</a></li></ul></li><li><a href="#visual-scene-understanding" class="table-of-contents__link toc-highlight">Visual Scene Understanding</a><ul><li><a href="#object-detection-and-recognition" class="table-of-contents__link toc-highlight">Object Detection and Recognition</a></li></ul></li><li><a href="#voice-command-pipeline-integration" class="table-of-contents__link toc-highlight">Voice Command Pipeline Integration</a></li><li><a href="#integration-with-humanoid-robot-control" class="table-of-contents__link toc-highlight">Integration with Humanoid Robot Control</a><ul><li><a href="#humanoid-action-execution" class="table-of-contents__link toc-highlight">Humanoid Action Execution</a></li></ul></li><li><a href="#practice-tasks" class="table-of-contents__link toc-highlight">Practice Tasks</a></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Chapters</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/my_book/docs/chapter1-ros2">ROS 2 Fundamentals</a></li><li class="footer__item"><a class="footer__link-item" href="/my_book/docs/chapter2-digital-twin">Digital Twin</a></li><li class="footer__item"><a class="footer__link-item" href="/my_book/docs/chapter3-isaac">AI-Robot Brain</a></li><li class="footer__item"><a class="footer__link-item" href="/my_book/docs/chapter4-vla">Vision-Language-Action</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/ros2" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://answers.ros.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS Answers<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://developer.nvidia.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Developer<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Ayesha788/my_book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2025 Physical AI & Humanoid Robotics Hackathon Project. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>