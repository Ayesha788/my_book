"use strict";(globalThis.webpackChunkmy_book_site=globalThis.webpackChunkmy_book_site||[]).push([[646],{6829:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"capstone","title":"Capstone Project: Integrated Physical AI & Humanoid Robotics System","description":"Project Overview","source":"@site/docs/capstone.md","sourceDirName":".","slug":"/capstone","permalink":"/my_book/docs/capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/Ayesha788/my_book/tree/main/docs/capstone.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Glossary of Terms","permalink":"/my_book/docs/glossary"}}');var o=t(4848),i=t(8453);const r={},a="Capstone Project: Integrated Physical AI & Humanoid Robotics System",c={},l=[{value:"Project Overview",id:"project-overview",level:2},{value:"Project Requirements",id:"project-requirements",level:2},{value:"Core Functionality",id:"core-functionality",level:3},{value:"Technical Constraints",id:"technical-constraints",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"High-Level Design",id:"high-level-design",level:3},{value:"Component Breakdown",id:"component-breakdown",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Project Setup",id:"step-1-project-setup",level:3},{value:"Step 2: Define Custom Messages",id:"step-2-define-custom-messages",level:3},{value:"Step 3: Voice Command Node",id:"step-3-voice-command-node",level:3},{value:"Step 4: Visual Perception Node",id:"step-4-visual-perception-node",level:3},{value:"Step 5: Action Planning Node",id:"step-5-action-planning-node",level:3},{value:"Step 6: Launch File",id:"step-6-launch-file",level:3},{value:"Step 7: Simulation Environment",id:"step-7-simulation-environment",level:3},{value:"Testing the System",id:"testing-the-system",level:2},{value:"Running the Complete System",id:"running-the-complete-system",level:3},{value:"Verification Steps",id:"verification-steps",level:3},{value:"Extensions and Improvements",id:"extensions-and-improvements",level:2},{value:"Advanced Features",id:"advanced-features",level:3},{value:"Performance Optimizations",id:"performance-optimizations",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"capstone-project-integrated-physical-ai--humanoid-robotics-system",children:"Capstone Project: Integrated Physical AI & Humanoid Robotics System"})}),"\n",(0,o.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,o.jsx)(n.p,{children:"The capstone project brings together all the concepts learned throughout this book into a comprehensive Physical AI & Humanoid Robotics system. You'll create a complete application that integrates ROS 2 fundamentals, digital twin simulation, AI-powered control, and voice-driven interaction."}),"\n",(0,o.jsx)(n.h2,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,o.jsx)(n.h3,{id:"core-functionality",children:"Core Functionality"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 Integration"}),": Implement a complete ROS 2 system with multiple nodes communicating via topics and services"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulation Environment"}),": Create a Gazebo simulation with a humanoid robot and interactive environment"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"AI Control"}),": Develop AI algorithms for perception, planning, and control"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Interface"}),": Implement voice command recognition and execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Humanoid Control"}),": Execute complex humanoid robot behaviors"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"technical-constraints",children:"Technical Constraints"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use ROS 2 Humble Hawksbill"}),"\n",(0,o.jsx)(n.li,{children:"Implement GPU-accelerated processing where possible"}),"\n",(0,o.jsx)(n.li,{children:"Ensure system runs in simulation before considering real hardware"}),"\n",(0,o.jsx)(n.li,{children:"Include comprehensive error handling and safety measures"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,o.jsx)(n.h3,{id:"high-level-design",children:"High-Level Design"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Voice Input   \u2502\u2500\u2500\u2500\u25b6\u2502  NLP Processing  \u2502\u2500\u2500\u2500\u25b6\u2502 Action Planning \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502                          \u2502\n                              \u25bc                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Camera Feed   \u2502\u2500\u2500\u2500\u25b6\u2502 Visual Analysis  \u2502\u2500\u2500\u2500\u25b6\u2502 Motion Control  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502                          \u2502\n                              \u25bc                          \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502           Robot Platform              \u2502\n                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n                    \u2502  \u2502 Simulation  \u2502  \u2502 Real Hardware   \u2502  \u2502\n                    \u2502  \u2502   (Gazebo)  \u2502  \u2502   (Optional)    \u2502  \u2502\n                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(n.h3,{id:"component-breakdown",children:"Component Breakdown"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Processing Module"}),": Handles speech-to-text conversion and command interpretation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Visual Perception Module"}),": Processes camera feeds for object detection and scene understanding"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Planning Module"}),": Combines voice and visual inputs to determine appropriate actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Motion Control Module"}),": Translates high-level actions into low-level joint commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robot Platform"}),": Either simulated in Gazebo or on real hardware"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,o.jsx)(n.h3,{id:"step-1-project-setup",children:"Step 1: Project Setup"}),"\n",(0,o.jsx)(n.p,{children:"First, create the package structure for your capstone project:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python capstone_project --dependencies rclpy std_msgs sensor_msgs geometry_msgs nav_msgs message_generation\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-2-define-custom-messages",children:"Step 2: Define Custom Messages"}),"\n",(0,o.jsx)(n.p,{children:"Create a message file for the capstone project:"}),"\n",(0,o.jsxs)(n.p,{children:["File: ",(0,o.jsx)(n.code,{children:"capstone_project/msg/RobotCommand.msg"})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"string command_type\nstring target_object\nfloat64[] target_position\nstring description\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Update your ",(0,o.jsx)(n.code,{children:"package.xml"})," to include message generation dependencies:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:"<depend>message_runtime</depend>\n<depend>builtin_interfaces</depend>\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-3-voice-command-node",children:"Step 3: Voice Command Node"}),"\n",(0,o.jsx)(n.p,{children:"Create the main voice processing node:"}),"\n",(0,o.jsxs)(n.p,{children:["File: ",(0,o.jsx)(n.code,{children:"capstone_project/voice_processor.py"})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport whisper\nimport pyaudio\nimport wave\nimport numpy as np\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom capstone_project.msg import RobotCommand\nimport threading\nimport queue\n\nclass VoiceProcessorNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_processor\')\n\n        # Initialize Whisper model\n        self.model = whisper.load_model("base")\n\n        # Audio parameters\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 44100\n        self.record_seconds = 3\n\n        # Publishers and subscribers\n        self.command_pub = self.create_publisher(RobotCommand, \'robot_command\', 10)\n        self.status_pub = self.create_publisher(String, \'voice_status\', 10)\n\n        # Initialize audio\n        self.audio = pyaudio.PyAudio()\n\n        # Create a thread for continuous listening\n        self.listening = True\n        self.command_queue = queue.Queue()\n\n        # Start listening thread\n        self.listen_thread = threading.Thread(target=self.continuous_listening)\n        self.listen_thread.start()\n\n        self.get_logger().info("Voice Processor initialized")\n\n    def continuous_listening(self):\n        """Continuously listen for voice commands"""\n        while self.listening:\n            try:\n                # Record audio\n                frames = self.record_audio()\n\n                # Transcribe\n                command_text = self.transcribe_audio(frames)\n\n                if command_text:\n                    self.get_logger().info(f"Recognized: {command_text}")\n\n                    # Process and publish command\n                    self.process_command(command_text)\n\n            except Exception as e:\n                self.get_logger().error(f"Error in listening loop: {e}")\n\n            # Small delay to prevent excessive CPU usage\n            time.sleep(0.1)\n\n    def record_audio(self):\n        """Record audio from microphone"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        frames = []\n        for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\n            data = stream.read(self.chunk)\n            frames.append(data)\n\n        stream.stop_stream()\n        stream.close()\n\n        return frames\n\n    def transcribe_audio(self, frames):\n        """Transcribe audio using Whisper"""\n        # Save to temp file\n        filename = "/tmp/temp_recording.wav"\n        wf = wave.open(filename, \'wb\')\n        wf.setnchannels(self.channels)\n        wf.setsampwidth(self.audio.get_sample_size(self.format))\n        wf.setframerate(self.rate)\n        wf.writeframes(b\'\'.join(frames))\n        wf.close()\n\n        # Transcribe\n        result = self.model.transcribe(filename)\n        transcription = result["text"]\n\n        # Clean up\n        import os\n        os.remove(filename)\n\n        return transcription.strip()\n\n    def process_command(self, command_text):\n        """Process the recognized command"""\n        command_text = command_text.lower().strip()\n\n        # Create robot command message\n        cmd_msg = RobotCommand()\n        cmd_msg.description = command_text\n\n        if "move forward" in command_text:\n            cmd_msg.command_type = "MOVE"\n            cmd_msg.target_position = [1.0, 0.0, 0.0]  # Move 1m forward\n        elif "turn left" in command_text:\n            cmd_msg.command_type = "ROTATE"\n            cmd_msg.target_position = [0.0, 0.0, 1.57]  # 90 degrees left\n        elif "turn right" in command_text:\n            cmd_msg.command_type = "ROTATE"\n            cmd_msg.target_position = [0.0, 0.0, -1.57]  # 90 degrees right\n        elif "wave" in command_text or "hello" in command_text:\n            cmd_msg.command_type = "GESTURE"\n            cmd_msg.target_object = "wave"\n        elif "pick up" in command_text:\n            obj = command_text.replace("pick up", "").strip()\n            cmd_msg.command_type = "GRASP"\n            cmd_msg.target_object = obj\n        else:\n            cmd_msg.command_type = "UNKNOWN"\n            cmd_msg.description = f"Unknown command: {command_text}"\n\n        self.command_pub.publish(cmd_msg)\n\n    def destroy_node(self):\n        """Clean up resources"""\n        self.listening = False\n        if hasattr(self, \'listen_thread\'):\n            self.listen_thread.join()\n        if hasattr(self, \'audio\'):\n            self.audio.terminate()\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceProcessorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"step-4-visual-perception-node",children:"Step 4: Visual Perception Node"}),"\n",(0,o.jsx)(n.p,{children:"Create the visual perception component:"}),"\n",(0,o.jsxs)(n.p,{children:["File: ",(0,o.jsx)(n.code,{children:"capstone_project/visual_perceptor.py"})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport cv2\nimport numpy as np\nfrom cv_bridge import CvBridge\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom std_msgs.msg import String\nfrom capstone_project.msg import RobotCommand\nimport threading\n\nclass VisualPerceptorNode(Node):\n    def __init__(self):\n        super().__init__('visual_perceptor')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10\n        )\n\n        self.command_sub = self.create_subscription(\n            RobotCommand, 'robot_command', self.command_callback, 10\n        )\n\n        # Publishers\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, 'object_detections', 10\n        )\n\n        self.status_pub = self.create_publisher(String, 'vision_status', 10)\n\n        # Object detection model (using OpenCV DNN for simplicity)\n        # In practice, you'd use a more sophisticated model like YOLO\n        self.detector = cv2.dnn_DetectionModel()\n\n        self.get_logger().info(\"Visual Perceptor initialized\")\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera images\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Perform object detection\n            detections = self.detect_objects(cv_image)\n\n            # Publish detections\n            self.publish_detections(detections, msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error processing image: {e}\")\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects in the image\"\"\"\n        # For this example, we'll use a simple color-based detection\n        # In practice, use a deep learning model\n        height, width = image.shape[:2]\n\n        # Detect red objects (potential targets)\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        lower_red = np.array([0, 120, 70])\n        upper_red = np.array([10, 255, 255])\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        # Upper red range (for colors that wrap around)\n        lower_red = np.array([170, 120, 70])\n        upper_red = np.array([180, 255, 255])\n        mask2 = cv2.inRange(hsv, lower_red, upper_red)\n\n        mask = mask1 + mask2\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        detections = []\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area > 500:  # Filter out small detections\n                x, y, w, h = cv2.boundingRect(contour)\n                detections.append({\n                    'class': 'red_object',\n                    'confidence': 0.8,\n                    'bbox': [x, y, w, h],\n                    'center': [x + w/2, y + h/2]\n                })\n\n        return detections\n\n    def publish_detections(self, detections, header):\n        \"\"\"Publish object detections\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for det in detections:\n            detection = Detection2D()\n            detection.header = header\n\n            # Set bounding box\n            detection.bbox.size_x = det['bbox'][2]\n            detection.bbox.size_y = det['bbox'][3]\n            detection.bbox.center.x = det['bbox'][0] + det['bbox'][2] / 2\n            detection.bbox.center.y = det['bbox'][1] + det['bbox'][3] / 2\n\n            # Set classification\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.id = det['class']\n            hypothesis.score = det['confidence']\n            detection.results.append(hypothesis)\n\n            detection_array.detections.append(detection)\n\n        self.detection_pub.publish(detection_array)\n\n    def command_callback(self, msg):\n        \"\"\"Process commands that require visual confirmation\"\"\"\n        if msg.command_type == \"GRASP\" and msg.target_object == \"red_object\":\n            # Look for red objects to grasp\n            self.get_logger().info(\"Looking for red object to grasp\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisualPerceptorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-5-action-planning-node",children:"Step 5: Action Planning Node"}),"\n",(0,o.jsx)(n.p,{children:"Create the cognitive planning component:"}),"\n",(0,o.jsxs)(n.p,{children:["File: ",(0,o.jsx)(n.code,{children:"capstone_project/action_planner.py"})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom capstone_project.msg import RobotCommand\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nimport json\n\nclass ActionPlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'action_planner\')\n\n        # Subscribers\n        self.command_sub = self.create_subscription(\n            RobotCommand, \'robot_command\', self.command_callback, 10\n        )\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.status_pub = self.create_publisher(String, \'planning_status\', 10)\n\n        # Robot state\n        self.robot_position = [0.0, 0.0, 0.0]  # x, y, theta\n        self.is_moving = False\n\n        self.get_logger().info("Action Planner initialized")\n\n    def command_callback(self, msg):\n        """Process incoming robot commands"""\n        self.get_logger().info(f"Planning action: {msg.command_type} - {msg.description}")\n\n        if msg.command_type == "MOVE":\n            self.execute_move_command(msg)\n        elif msg.command_type == "ROTATE":\n            self.execute_rotate_command(msg)\n        elif msg.command_type == "GESTURE":\n            self.execute_gesture_command(msg)\n        elif msg.command_type == "GRASP":\n            self.execute_grasp_command(msg)\n        else:\n            self.get_logger().warn(f"Unknown command type: {msg.command_type}")\n\n    def execute_move_command(self, msg):\n        """Execute movement command"""\n        if len(msg.target_position) >= 3:\n            linear_x = msg.target_position[0]\n            linear_y = msg.target_position[1] if len(msg.target_position) > 1 else 0.0\n\n            twist = Twist()\n            twist.linear.x = linear_x\n            twist.linear.y = linear_y\n            twist.angular.z = 0.0  # No rotation during linear movement\n\n            self.cmd_vel_pub.publish(twist)\n\n            # Update status\n            status_msg = String()\n            status_msg.data = f"Moving: x={linear_x}, y={linear_y}"\n            self.status_pub.publish(status_msg)\n\n    def execute_rotate_command(self, msg):\n        """Execute rotation command"""\n        if len(msg.target_position) >= 3:\n            angular_z = msg.target_position[2]\n\n            twist = Twist()\n            twist.linear.x = 0.0\n            twist.linear.y = 0.0\n            twist.angular.z = angular_z\n\n            self.cmd_vel_pub.publish(twist)\n\n            # Update status\n            status_msg = String()\n            status_msg.data = f"Rotating: {angular_z} rad/s"\n            self.status_pub.publish(status_msg)\n\n    def execute_gesture_command(self, msg):\n        """Execute gesture command (placeholder for humanoid actions)"""\n        self.get_logger().info(f"Executing gesture: {msg.target_object}")\n\n        # In a real implementation, this would send commands to joint controllers\n        status_msg = String()\n        status_msg.data = f"Executing gesture: {msg.target_object}"\n        self.status_pub.publish(status_msg)\n\n    def execute_grasp_command(self, msg):\n        """Execute grasping command"""\n        self.get_logger().info(f"Attempting to grasp: {msg.target_object}")\n\n        # This would integrate with manipulation stack in a real system\n        status_msg = String()\n        status_msg.data = f"Attempting to grasp: {msg.target_object}"\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionPlannerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"step-6-launch-file",children:"Step 6: Launch File"}),"\n",(0,o.jsx)(n.p,{children:"Create a launch file to start all nodes:"}),"\n",(0,o.jsxs)(n.p,{children:["File: ",(0,o.jsx)(n.code,{children:"capstone_project/launch/capstone_launch.py"})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    # Get package directory\n    pkg_capstone = get_package_share_directory('capstone_project')\n\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n\n    # Voice processor node\n    voice_processor = Node(\n        package='capstone_project',\n        executable='voice_processor',\n        name='voice_processor',\n        output='screen',\n    )\n\n    # Visual perceptor node\n    visual_perceptor = Node(\n        package='capstone_project',\n        executable='visual_perceptor',\n        name='visual_perceptor',\n        output='screen',\n    )\n\n    # Action planner node\n    action_planner = Node(\n        package='capstone_project',\n        executable='action_planner',\n        name='action_planner',\n        output='screen',\n    )\n\n    # Combined launch description\n    ld = LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n        voice_processor,\n        visual_perceptor,\n        action_planner,\n    ])\n\n    return ld\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-7-simulation-environment",children:"Step 7: Simulation Environment"}),"\n",(0,o.jsx)(n.p,{children:"Create a Gazebo world file for the capstone project:"}),"\n",(0,o.jsxs)(n.p,{children:["File: ",(0,o.jsx)(n.code,{children:"capstone_project/worlds/capstone_world.sdf"})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0" ?>\n<sdf version="1.7">\n  <world name="capstone_world">\n    \x3c!-- Physics --\x3e\n    <physics type="ode">\n      <gravity>0 0 -9.8</gravity>\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1</real_time_factor>\n      <real_time_update_rate>1000</real_time_update_rate>\n    </physics>\n\n    \x3c!-- Ground plane --\x3e\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n\n    \x3c!-- Lighting --\x3e\n    <include>\n      <uri>model://sun</uri>\n    </include>\n\n    \x3c!-- Simple room with objects --\x3e\n    <model name="wall_1">\n      <pose>0 5 0 0 0 0</pose>\n      <link name="link">\n        <collision name="collision">\n          <geometry>\n            <box>\n              <size>10 0.2 2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box>\n              <size>10 0.2 2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.5 0.5 0.5 1</ambient>\n            <diffuse>0.8 0.8 0.8 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>\n\n    \x3c!-- Red object for detection --\x3e\n    <model name="red_cube">\n      <pose>2 0 0.5 0 0 0</pose>\n      <link name="link">\n        <collision name="collision">\n          <geometry>\n            <box>\n              <size>0.2 0.2 0.2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box>\n              <size>0.2 0.2 0.2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>1 0 0 1</ambient>\n            <diffuse>1 0 0 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>\n\n    \x3c!-- Blue object for detection --\x3e\n    <model name="blue_cube">\n      <pose>-2 0 0.5 0 0 0</pose>\n      <link name="link">\n        <collision name="collision">\n          <geometry>\n            <box>\n              <size>0.2 0.2 0.2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box>\n              <size>0.2 0.2 0.2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0 0 1 1</ambient>\n            <diffuse>0 0 1 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>\n  </world>\n</sdf>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"testing-the-system",children:"Testing the System"}),"\n",(0,o.jsx)(n.h3,{id:"running-the-complete-system",children:"Running the Complete System"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Build your workspace:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select capstone_project\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Launch the simulation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# In one terminal, start Gazebo\ngazebo --verbose ~/ros2_ws/src/capstone_project/worlds/capstone_world.sdf\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"In another terminal, launch the capstone nodes:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 launch capstone_project capstone_launch.py\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Test voice commands through the voice processor node."}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"verification-steps",children:"Verification Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Recognition"}),": Verify that voice commands are properly recognized and converted to text"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Visual Detection"}),": Check that objects are detected in the camera feed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Command Mapping"}),": Confirm that voice commands are correctly mapped to robot actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Motion Execution"}),": Verify that the robot executes requested movements"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration"}),": Test the complete pipeline from voice command to robot action"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"extensions-and-improvements",children:"Extensions and Improvements"}),"\n",(0,o.jsx)(n.h3,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-modal Learning"}),": Train a model that jointly processes voice and visual inputs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Semantic Mapping"}),": Create semantic maps of the environment with object locations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Adaptive Interaction"}),": Implement learning from user feedback to improve interaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-robot Coordination"}),": Extend to multiple robots working together"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"performance-optimizations",children:"Performance Optimizations"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time Processing"}),": Optimize for real-time performance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Edge Deployment"}),": Optimize models for deployment on robot hardware"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety Mechanisms"}),": Implement safety checks and emergency stops"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Add error recovery and fallback mechanisms"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(n.p,{children:"The capstone project demonstrates the integration of all key components covered in this book:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"ROS 2 for system integration and communication"}),"\n",(0,o.jsx)(n.li,{children:"Digital twin simulation for testing and development"}),"\n",(0,o.jsx)(n.li,{children:"AI algorithms for perception and decision-making"}),"\n",(0,o.jsx)(n.li,{children:"Voice interfaces for natural human-robot interaction"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This complete system serves as a foundation for more advanced humanoid robotics applications and provides a framework for extending capabilities in specialized domains."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const o={},i=s.createContext(o);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);