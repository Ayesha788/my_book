"use strict";(globalThis.webpackChunkmy_book_site=globalThis.webpackChunkmy_book_site||[]).push([[594],{7217:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>f,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"chapter9-learning","title":"Chapter 9: Learning and Adaptation for Humanoid Robots","description":"Introduction to Robot Learning","source":"@site/docs/chapter9-learning.md","sourceDirName":".","slug":"/chapter9-learning","permalink":"/my_book/docs/chapter9-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/Ayesha788/my_book/tree/main/docs/chapter9-learning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 8: Multi-Robot Coordination for Humanoid Systems","permalink":"/my_book/docs/chapter8-multirobot"},"next":{"title":"Chapter 10: Human-Robot Interaction for Humanoid Systems","permalink":"/my_book/docs/chapter10-hri"}}');var a=t(4848),i=t(8453);const s={},o="Chapter 9: Learning and Adaptation for Humanoid Robots",l={},c=[{value:"Introduction to Robot Learning",id:"introduction-to-robot-learning",level:2},{value:"Machine Learning in Robotics",id:"machine-learning-in-robotics",level:2},{value:"Supervised Learning for Robot Perception",id:"supervised-learning-for-robot-perception",level:3},{value:"Reinforcement Learning for Robot Control",id:"reinforcement-learning-for-robot-control",level:3},{value:"Imitation Learning for Humanoid Robots",id:"imitation-learning-for-humanoid-robots",level:2},{value:"Learning from Human Feedback",id:"learning-from-human-feedback",level:2},{value:"Adaptive Control Systems",id:"adaptive-control-systems",level:2},{value:"Online Learning and Adaptation",id:"online-learning-and-adaptation",level:2},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Learning Node Implementation",id:"learning-node-implementation",level:3},{value:"Challenges in Robot Learning",id:"challenges-in-robot-learning",level:2},{value:"Sample Efficiency",id:"sample-efficiency",level:3},{value:"Transfer Learning",id:"transfer-learning",level:3},{value:"Safety and Robustness",id:"safety-and-robustness",level:3},{value:"Practice Tasks",id:"practice-tasks",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-9-learning-and-adaptation-for-humanoid-robots",children:"Chapter 9: Learning and Adaptation for Humanoid Robots"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction-to-robot-learning",children:"Introduction to Robot Learning"}),"\n",(0,a.jsx)(n.p,{children:"Learning and adaptation are crucial for humanoid robots to operate effectively in dynamic environments. Unlike traditional robots programmed for specific tasks, humanoid robots must continuously adapt to new situations, learn from experience, and improve their performance over time."}),"\n",(0,a.jsx)(n.h2,{id:"machine-learning-in-robotics",children:"Machine Learning in Robotics"}),"\n",(0,a.jsx)(n.h3,{id:"supervised-learning-for-robot-perception",children:"Supervised Learning for Robot Perception"}),"\n",(0,a.jsx)(n.p,{children:"Supervised learning techniques are commonly used for perception tasks in humanoid robots:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\nclass RobotPerceptionNet(nn.Module):\n    def __init__(self, input_channels=3, num_classes=10):\n        super(RobotPerceptionNet, self).__init__()\n\n        # Convolutional layers for feature extraction\n        self.features = nn.Sequential(\n            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((4, 4))\n        )\n\n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(128 * 4 * 4, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\nclass RobotPerceptionLearner:\n    def __init__(self, num_classes=10):\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        self.model = RobotPerceptionNet(num_classes=num_classes).to(self.device)\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1)\n\n    def train(self, train_loader, epochs=10):\n        """Train the perception network"""\n        self.model.train()\n\n        for epoch in range(epochs):\n            running_loss = 0.0\n            correct = 0\n            total = 0\n\n            for batch_idx, (data, target) in enumerate(train_loader):\n                data, target = data.to(self.device), target.to(self.device)\n\n                self.optimizer.zero_grad()\n                output = self.model(data)\n                loss = self.criterion(output, target)\n                loss.backward()\n                self.optimizer.step()\n\n                running_loss += loss.item()\n                _, predicted = output.max(1)\n                total += target.size(0)\n                correct += predicted.eq(target).sum().item()\n\n                if batch_idx % 100 == 0:\n                    print(f\'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.3f}\')\n\n            accuracy = 100. * correct / total\n            print(f\'Epoch {epoch} completed. Accuracy: {accuracy:.2f}%\')\n\n            self.scheduler.step()\n\n    def predict(self, image_tensor):\n        """Make prediction on a single image"""\n        self.model.eval()\n        with torch.no_grad():\n            image_tensor = image_tensor.to(self.device)\n            output = self.model(image_tensor.unsqueeze(0))\n            probabilities = torch.softmax(output, dim=1)\n            predicted_class = torch.argmax(probabilities, dim=1)\n            confidence = torch.max(probabilities, dim=1)[0]\n\n        return predicted_class.item(), confidence.item()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"reinforcement-learning-for-robot-control",children:"Reinforcement Learning for Robot Control"}),"\n",(0,a.jsx)(n.p,{children:"Reinforcement learning is particularly powerful for humanoid robot control, allowing robots to learn complex behaviors through trial and error:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom collections import deque\nimport random\n\nclass ActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(ActorCritic, self).__init__()\n\n        # Shared feature extractor\n        self.shared = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Actor (policy network)\n        self.actor = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()  # Actions between -1 and 1\n        )\n\n        # Critic (value network)\n        self.critic = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, state):\n        features = self.shared(state)\n        action = self.actor(features)\n        value = self.critic(features)\n        return action, value\n\nclass PPOAgent:\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2):\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n        self.actor_critic = ActorCritic(state_dim, action_dim).to(self.device)\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.buffer = []\n\n    def select_action(self, state):\n        """Select action using current policy"""\n        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n\n        with torch.no_grad():\n            action_mean, _ = self.actor_critic(state)\n\n        # Add noise for exploration\n        noise = torch.randn_like(action_mean) * 0.1\n        action = torch.clamp(action_mean + noise, -1.0, 1.0)\n\n        return action.cpu().numpy()[0]\n\n    def compute_returns(self, rewards, dones):\n        """Compute discounted returns"""\n        returns = []\n        R = 0\n\n        for i in reversed(range(len(rewards))):\n            if dones[i]:\n                R = 0\n            R = rewards[i] + self.gamma * R\n            returns.insert(0, R)\n\n        return returns\n\n    def update(self, states, actions, rewards, dones):\n        """Update policy using PPO"""\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.FloatTensor(actions).to(self.device)\n        returns = self.compute_returns(rewards, dones)\n        returns = torch.FloatTensor(returns).to(self.device).unsqueeze(1)\n\n        # Current policy\n        curr_actions, curr_values = self.actor_critic(states)\n\n        # Compute advantages\n        advantages = returns - curr_values\n\n        # Compute loss\n        ratio = torch.exp(curr_actions - actions)  # Simplified for continuous actions\n        surr1 = ratio * advantages\n        surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n        actor_loss = -torch.min(surr1, surr2).mean()\n\n        critic_loss = F.mse_loss(curr_values, returns)\n\n        loss = actor_loss + 0.5 * critic_loss\n\n        # Update network\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"imitation-learning-for-humanoid-robots",children:"Imitation Learning for Humanoid Robots"}),"\n",(0,a.jsx)(n.p,{children:"Imitation learning allows humanoid robots to learn from human demonstrations:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ImitationLearner:\n    def __init__(self, state_dim, action_dim):\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n        # Behavior cloning network\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim),\n            nn.Tanh()\n        ).to(self.device)\n\n        self.optimizer = optim.Adam(self.network.parameters(), lr=0.001)\n        self.criterion = nn.MSELoss()\n\n        # Storage for demonstration data\n        self.demonstration_states = []\n        self.demonstration_actions = []\n\n    def add_demonstration(self, states, actions):\n        """Add demonstration data"""\n        self.demonstration_states.extend(states)\n        self.demonstration_actions.extend(actions)\n\n    def behavior_cloning_train(self, epochs=100):\n        """Train using behavior cloning"""\n        if len(self.demonstration_states) == 0:\n            print("No demonstration data available")\n            return\n\n        states = torch.FloatTensor(self.demonstration_states).to(self.device)\n        actions = torch.FloatTensor(self.demonstration_actions).to(self.device)\n\n        self.network.train()\n\n        for epoch in range(epochs):\n            self.optimizer.zero_grad()\n\n            predicted_actions = self.network(states)\n            loss = self.criterion(predicted_actions, actions)\n\n            loss.backward()\n            self.optimizer.step()\n\n            if epoch % 20 == 0:\n                print(f\'Epoch {epoch}, Loss: {loss.item():.4f}\')\n\n    def predict_action(self, state):\n        """Predict action for given state"""\n        self.network.eval()\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n            action = self.network(state_tensor)\n        return action.cpu().numpy()[0]\n\nclass DAgger(ImitationLearner):\n    """Dataset Aggregation algorithm for imitation learning"""\n\n    def __init__(self, state_dim, action_dim, env):\n        super().__init__(state_dim, action_dim)\n        self.env = env\n        self.expert_policy = None  # Function that takes state and returns action\n\n    def set_expert_policy(self, expert_policy):\n        """Set the expert policy for data collection"""\n        self.expert_policy = expert_policy\n\n    def dagger_train(self, iterations=10, episodes_per_iter=10):\n        """Train using DAgger algorithm"""\n        for iteration in range(iterations):\n            print(f\'DAgger iteration {iteration + 1}/{iterations}\')\n\n            # Collect data using current policy\n            states, actions = self.collect_data(episodes_per_iter)\n\n            # Add expert actions for these states\n            expert_actions = [self.expert_policy(s) for s in states]\n\n            # Add to demonstration dataset\n            self.demonstration_states.extend(states)\n            self.demonstration_actions.extend(expert_actions)\n\n            # Retrain behavior cloning model\n            self.behavior_cloning_train(epochs=50)\n\n    def collect_data(self, num_episodes):\n        """Collect data using current policy"""\n        all_states = []\n        all_actions = []\n\n        for _ in range(num_episodes):\n            state = self.env.reset()\n            done = False\n\n            while not done:\n                # Get action from current policy\n                action = self.predict_action(state)\n\n                all_states.append(state.copy())\n                all_actions.append(action.copy())\n\n                state, reward, done, _ = self.env.step(action)\n\n        return all_states, all_actions\n'})}),"\n",(0,a.jsx)(n.h2,{id:"learning-from-human-feedback",children:"Learning from Human Feedback"}),"\n",(0,a.jsx)(n.p,{children:"Humanoid robots can learn from explicit human feedback to improve their behavior:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class HumanFeedbackLearner:\n    def __init__(self, state_dim, action_dim):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n        # Policy network\n        self.policy = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim),\n            nn.Tanh()\n        ).to(self.device)\n\n        # Preference model for learning from comparisons\n        self.preference_model = nn.Sequential(\n            nn.Linear(state_dim * 2 + action_dim * 2, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()  # Probability that first trajectory is better\n        ).to(self.device)\n\n        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=0.001)\n        self.optimizer_pref = optim.Adam(self.preference_model.parameters(), lr=0.001)\n\n        self.feedback_buffer = []\n\n    def add_preference_feedback(self, traj1_states, traj1_actions, traj2_states, traj2_actions, preferred_traj=1):\n        """Add preference feedback where preferred_traj is 1 or 2"""\n        if preferred_traj == 1:\n            # traj1 is preferred over traj2\n            self.feedback_buffer.append((traj1_states, traj1_actions, traj2_states, traj2_actions, 1.0))\n        else:\n            # traj2 is preferred over traj1\n            self.feedback_buffer.append((traj2_states, traj2_actions, traj1_states, traj1_actions, 1.0))\n\n    def train_preference_model(self, epochs=50):\n        """Train the preference model"""\n        if len(self.feedback_buffer) == 0:\n            return\n\n        for epoch in range(epochs):\n            total_loss = 0\n\n            for (s1, a1, s2, a2, label) in self.feedback_buffer:\n                # Convert to tensors\n                s1_tensor = torch.FloatTensor(s1).mean(dim=0)  # Use average state\n                a1_tensor = torch.FloatTensor(a1).mean(dim=0)  # Use average action\n                s2_tensor = torch.FloatTensor(s2).mean(dim=0)\n                a2_tensor = torch.FloatTensor(a2).mean(dim=0)\n\n                # Concatenate state-action pairs\n                input_pair = torch.cat([s1_tensor, a1_tensor, s2_tensor, a2_tensor])\n\n                self.optimizer_pref.zero_grad()\n\n                pred_prob = self.preference_model(input_pair.unsqueeze(0))\n                true_label = torch.FloatTensor([label]).to(self.device)\n\n                loss = F.binary_cross_entropy(pred_prob, true_label)\n\n                loss.backward()\n                self.optimizer_pref.step()\n\n                total_loss += loss.item()\n\n    def get_reward_from_preference(self, state, action):\n        """Get reward signal from learned preference model"""\n        # This is a simplified version - in practice, you\'d need to evaluate\n        # the trajectory quality using the preference model\n        with torch.no_grad():\n            # Placeholder implementation\n            return 0.0  # Would use preference model to assign reward\n'})}),"\n",(0,a.jsx)(n.h2,{id:"adaptive-control-systems",children:"Adaptive Control Systems"}),"\n",(0,a.jsx)(n.p,{children:"Humanoid robots need adaptive control systems that can adjust to changing conditions:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class AdaptiveController:\n    def __init__(self, num_joints):\n        self.num_joints = num_joints\n\n        # Initial controller parameters\n        self.kp = np.array([100.0] * num_joints)  # Proportional gains\n        self.ki = np.array([10.0] * num_joints)   # Integral gains\n        self.kd = np.array([10.0] * num_joints)   # Derivative gains\n\n        # Adaptive parameters\n        self.error_history = deque(maxlen=100)\n        self.control_effort_history = deque(maxlen=100)\n\n        # Parameter bounds\n        self.kp_min = np.array([10.0] * num_joints)\n        self.kp_max = np.array([500.0] * num_joints)\n        self.ki_min = np.array([1.0] * num_joints)\n        self.ki_max = np.array([100.0] * num_joints)\n        self.kd_min = np.array([1.0] * num_joints)\n        self.kd_max = np.array([100.0] * num_joints)\n\n    def update_gains(self, current_error, dt):\n        """Adaptively update controller gains based on performance"""\n        self.error_history.append(np.abs(current_error))\n\n        if len(self.error_history) >= 10:\n            # Calculate error statistics\n            recent_error = np.mean(list(self.error_history)[-10:])\n\n            # Adjust gains based on error magnitude\n            for i in range(self.num_joints):\n                if recent_error[i] > 0.1:  # High error - increase gains\n                    self.kp[i] = min(self.kp[i] * 1.05, self.kp_max[i])\n                    self.ki[i] = min(self.ki[i] * 1.02, self.ki_max[i])\n                elif recent_error[i] < 0.01:  # Low error - decrease gains to reduce oscillation\n                    self.kp[i] = max(self.kp[i] * 0.95, self.kp_min[i])\n                    self.ki[i] = max(self.ki[i] * 0.98, self.ki_min[i])\n\n    def compute_control(self, desired_pos, current_pos, desired_vel, current_vel, dt):\n        """Compute adaptive PID control"""\n        # Calculate errors\n        pos_error = desired_pos - current_pos\n        vel_error = desired_vel - current_vel\n\n        # Update adaptive gains\n        self.update_gains(pos_error, dt)\n\n        # PID control with adaptive gains\n        proportional = self.kp * pos_error\n        integral = self.ki * np.array(self.error_history).sum(axis=0) * dt if len(self.error_history) > 0 else np.zeros_like(pos_error)\n        derivative = self.kd * vel_error\n\n        control_output = proportional + integral + derivative\n\n        return control_output\n\nclass ModelLearningSystem:\n    """System for learning and adapting robot dynamics models"""\n\n    def __init__(self, state_dim, action_dim):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n        # Dynamics model: predicts next state given current state and action\n        self.dynamics_model = nn.Sequential(\n            nn.Linear(state_dim + action_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, state_dim)\n        ).to(self.device)\n\n        self.optimizer = optim.Adam(self.dynamics_model.parameters(), lr=0.001)\n        self.criterion = nn.MSELoss()\n\n        self.experience_buffer = deque(maxlen=10000)\n\n    def add_experience(self, state, action, next_state):\n        """Add experience to buffer"""\n        self.experience_buffer.append((state, action, next_state))\n\n    def train_dynamics_model(self, batch_size=32, epochs=10):\n        """Train the dynamics model"""\n        if len(self.experience_buffer) < batch_size:\n            return\n\n        for epoch in range(epochs):\n            batch = random.sample(list(self.experience_buffer), batch_size)\n\n            states = torch.FloatTensor([exp[0] for exp in batch]).to(self.device)\n            actions = torch.FloatTensor([exp[1] for exp in batch]).to(self.device)\n            next_states = torch.FloatTensor([exp[2] for exp in batch]).to(self.device)\n\n            # Concatenate state and action\n            state_action = torch.cat([states, actions], dim=1)\n\n            self.optimizer.zero_grad()\n\n            predicted_next_states = self.dynamics_model(state_action)\n            loss = self.criterion(predicted_next_states, next_states)\n\n            loss.backward()\n            self.optimizer.step()\n\n    def predict_next_state(self, state, action):\n        """Predict next state given current state and action"""\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n            action_tensor = torch.FloatTensor(action).to(self.device).unsqueeze(0)\n\n            state_action = torch.cat([state_tensor, action_tensor], dim=1)\n            predicted_state = self.dynamics_model(state_action)\n\n        return predicted_state.cpu().numpy()[0]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"online-learning-and-adaptation",children:"Online Learning and Adaptation"}),"\n",(0,a.jsx)(n.p,{children:"Humanoid robots need to learn and adapt in real-time during operation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class OnlineLearner:\n    def __init__(self, num_features=100):\n        self.num_features = num_features\n        self.learning_rate = 0.01\n\n        # Online learning parameters\n        self.weights = np.random.normal(0, 0.1, num_features)\n        self.feature_buffer = deque(maxlen=1000)\n        self.reward_buffer = deque(maxlen=1000)\n\n        # For incremental updates\n        self.feature_mean = np.zeros(num_features)\n        self.feature_var = np.ones(num_features)\n        self.sample_count = 0\n\n    def extract_features(self, state, action):\n        """Extract features from state-action pair"""\n        # Simple feature extraction - in practice, this would be more sophisticated\n        features = np.concatenate([\n            state.flatten(),\n            action.flatten(),\n            np.sin(state.flatten()),\n            np.cos(action.flatten())\n        ])\n\n        # Ensure we have the right number of features\n        if len(features) > self.num_features:\n            features = features[:self.num_features]\n        elif len(features) < self.num_features:\n            features = np.pad(features, (0, self.num_features - len(features)))\n\n        return features\n\n    def update(self, state, action, reward):\n        """Update the learner with new experience"""\n        features = self.extract_features(state, action)\n\n        # Normalize features incrementally\n        if self.sample_count == 0:\n            self.feature_mean = features.copy()\n            self.feature_var = np.ones_like(features) * 0.1\n        else:\n            # Incremental mean and variance update\n            self.feature_mean = (self.feature_mean * self.sample_count + features) / (self.sample_count + 1)\n            self.feature_var = (self.feature_var * self.sample_count + (features - self.feature_mean)**2) / (self.sample_count + 1)\n\n        self.sample_count += 1\n\n        # Normalize features\n        normalized_features = (features - self.feature_mean) / (np.sqrt(self.feature_var) + 1e-8)\n\n        # Store for learning\n        self.feature_buffer.append(normalized_features)\n        self.reward_buffer.append(reward)\n\n        # Update weights using stochastic gradient descent\n        if len(self.feature_buffer) > 1:\n            # Use the most recent experience\n            recent_features = self.feature_buffer[-1]\n            recent_reward = self.reward_buffer[-1]\n\n            # Compute prediction error\n            prediction = np.dot(self.weights, recent_features)\n            error = recent_reward - prediction\n\n            # Update weights\n            self.weights += self.learning_rate * error * recent_features\n\n    def predict_value(self, state, action):\n        """Predict the value of a state-action pair"""\n        features = self.extract_features(state, action)\n        normalized_features = (features - self.feature_mean) / (np.sqrt(self.feature_var) + 1e-8)\n        return np.dot(self.weights, normalized_features)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,a.jsx)(n.h3,{id:"learning-node-implementation",children:"Learning Node Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32MultiArray\nfrom sensor_msgs.msg import JointState\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import Float32\n\nclass LearningNode(Node):\n    def __init__(self):\n        super().__init__(\'learning_node\')\n\n        # Publishers\n        self.performance_pub = self.create_publisher(Float32, \'learning_performance\', 10)\n        self.adaptation_pub = self.create_publisher(Float32MultiArray, \'adaptation_params\', 10)\n\n        # Subscribers\n        self.joint_sub = self.create_subscription(\n            JointState, \'joint_states\', self.joint_callback, 10\n        )\n        self.task_sub = self.create_subscription(\n            Float32MultiArray, \'task_performance\', self.task_callback, 10\n        )\n\n        # Initialize learning components\n        self.imitation_learner = ImitationLearner(state_dim=12, action_dim=6)  # Example dimensions\n        self.adaptive_controller = AdaptiveController(num_joints=6)\n        self.online_learner = OnlineLearner(num_features=50)\n\n        # Timer for learning updates\n        self.learning_timer = self.create_timer(1.0, self.learning_step)\n\n        # Internal state\n        self.current_joint_states = None\n        self.task_performance = 0.0\n\n        self.get_logger().info(\'Learning node initialized\')\n\n    def joint_callback(self, msg: JointState):\n        """Update with current joint states"""\n        self.current_joint_states = np.array(msg.position)\n\n    def task_callback(self, msg: Float32MultiArray):\n        """Update with task performance feedback"""\n        if len(msg.data) > 0:\n            self.task_performance = msg.data[0]\n\n    def learning_step(self):\n        """Main learning step"""\n        if self.current_joint_states is not None:\n            # Example: update online learner with current performance\n            dummy_action = np.zeros(6)  # Placeholder action\n            self.online_learner.update(\n                self.current_joint_states,\n                dummy_action,\n                self.task_performance\n            )\n\n            # Publish performance\n            perf_msg = Float32()\n            perf_msg.data = self.task_performance\n            self.performance_pub.publish(perf_msg)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"challenges-in-robot-learning",children:"Challenges in Robot Learning"}),"\n",(0,a.jsx)(n.h3,{id:"sample-efficiency",children:"Sample Efficiency"}),"\n",(0,a.jsx)(n.p,{children:"Robot learning faces significant sample efficiency challenges:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Physical robots are expensive to operate"}),"\n",(0,a.jsx)(n.li,{children:"Each learning trial takes time and energy"}),"\n",(0,a.jsx)(n.li,{children:"Safety constraints limit exploration"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"transfer-learning",children:"Transfer Learning"}),"\n",(0,a.jsx)(n.p,{children:"Transferring learned behaviors across different robots or environments:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Domain adaptation techniques"}),"\n",(0,a.jsx)(n.li,{children:"Sim-to-real transfer"}),"\n",(0,a.jsx)(n.li,{children:"Multi-task learning"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"safety-and-robustness",children:"Safety and Robustness"}),"\n",(0,a.jsx)(n.p,{children:"Ensuring learned behaviors are safe and robust:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Safe exploration strategies"}),"\n",(0,a.jsx)(n.li,{children:"Robustness to environmental changes"}),"\n",(0,a.jsx)(n.li,{children:"Failure detection and recovery"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"practice-tasks",children:"Practice Tasks"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement a simple imitation learning algorithm for a humanoid robot task"}),"\n",(0,a.jsx)(n.li,{children:"Create a reinforcement learning environment for humanoid robot control"}),"\n",(0,a.jsx)(n.li,{children:"Develop an adaptive controller that adjusts to changing loads"}),"\n",(0,a.jsx)(n.li,{children:"Design a human feedback system for robot learning"}),"\n",(0,a.jsx)(n.li,{children:"Test learning algorithms in simulation with humanoid robots"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Learning and adaptation are essential capabilities for humanoid robots to operate effectively in dynamic environments. By implementing various learning techniques - from supervised learning for perception to reinforcement learning for control, and from imitation learning to human feedback integration - humanoid robots can continuously improve their performance and adapt to new situations."})]})}function f(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var r=t(6540);const a={},i=r.createContext(a);function s(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);