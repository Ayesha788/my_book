"use strict";(globalThis.webpackChunkmy_book_site=globalThis.webpackChunkmy_book_site||[]).push([[473],{574:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"chapter4-vla","title":"Chapter 4: Vision-Language-Action (VLA) - Converting Voice Commands to Robot Actions","description":"Introduction to Vision-Language-Action Systems","source":"@site/docs/chapter4-vla.md","sourceDirName":".","slug":"/chapter4-vla","permalink":"/my_book/docs/chapter4-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/Ayesha788/my_book/tree/main/docs/chapter4-vla.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: AI-Robot Brain - NVIDIA Isaac Integration","permalink":"/my_book/docs/chapter3-isaac"},"next":{"title":"Chapter 5: Path Planning and Navigation for Humanoid Robots","permalink":"/my_book/docs/chapter5-path-planning"}}');var i=t(4848),s=t(8453);const a={},r="Chapter 4: Vision-Language-Action (VLA) - Converting Voice Commands to Robot Actions",c={},l=[{value:"Introduction to Vision-Language-Action Systems",id:"introduction-to-vision-language-action-systems",level:2},{value:"Architecture of VLA Systems",id:"architecture-of-vla-systems",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:3},{value:"Voice Command Processing with OpenAI Whisper",id:"voice-command-processing-with-openai-whisper",level:2},{value:"Installing Whisper",id:"installing-whisper",level:3},{value:"Basic Whisper Integration",id:"basic-whisper-integration",level:3},{value:"Natural Language to ROS 2 Action Mapping",id:"natural-language-to-ros-2-action-mapping",level:2},{value:"Intent Recognition and Action Mapping",id:"intent-recognition-and-action-mapping",level:3},{value:"Visual Scene Understanding",id:"visual-scene-understanding",level:2},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:3},{value:"Voice Command Pipeline Integration",id:"voice-command-pipeline-integration",level:2},{value:"Integration with Humanoid Robot Control",id:"integration-with-humanoid-robot-control",level:2},{value:"Humanoid Action Execution",id:"humanoid-action-execution",level:3},{value:"Practice Tasks",id:"practice-tasks",level:2},{value:"Summary",id:"summary",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-4-vision-language-action-vla---converting-voice-commands-to-robot-actions",children:"Chapter 4: Vision-Language-Action (VLA) - Converting Voice Commands to Robot Actions"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-vision-language-action-systems",children:"Introduction to Vision-Language-Action Systems"}),"\n",(0,i.jsx)(e.p,{children:"Vision-Language-Action (VLA) systems represent the cutting edge of human-robot interaction, enabling robots to understand natural language commands, perceive their environment visually, and execute appropriate physical actions. This convergence of computer vision, natural language processing, and robotics creates intuitive interfaces that allow humans to interact with robots using everyday language."}),"\n",(0,i.jsx)(e.p,{children:"For humanoid robots, VLA systems enable:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Natural voice command interpretation"}),"\n",(0,i.jsx)(e.li,{children:"Visual scene understanding"}),"\n",(0,i.jsx)(e.li,{children:"Cognitive planning and task execution"}),"\n",(0,i.jsx)(e.li,{children:"Adaptive behavior based on environmental context"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"architecture-of-vla-systems",children:"Architecture of VLA Systems"}),"\n",(0,i.jsx)(e.h3,{id:"core-components",children:"Core Components"}),"\n",(0,i.jsx)(e.p,{children:"A typical VLA system consists of:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Speech Recognition"}),": Converting voice commands to text"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Natural Language Understanding"}),": Interpreting the meaning of commands"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Visual Perception"}),": Understanding the current environment"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cognitive Planning"}),": Determining appropriate actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action Execution"}),": Sending commands to robot actuators"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,i.jsx)(e.p,{children:"VLA systems integrate with ROS 2 through:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Message passing for sensor data and commands"}),"\n",(0,i.jsx)(e.li,{children:"Action servers for long-running tasks"}),"\n",(0,i.jsx)(e.li,{children:"Services for immediate queries"}),"\n",(0,i.jsx)(e.li,{children:"Parameter servers for configuration"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"voice-command-processing-with-openai-whisper",children:"Voice Command Processing with OpenAI Whisper"}),"\n",(0,i.jsx)(e.p,{children:"OpenAI Whisper is a state-of-the-art speech recognition model that can transcribe speech to text with high accuracy across multiple languages."}),"\n",(0,i.jsx)(e.h3,{id:"installing-whisper",children:"Installing Whisper"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"pip install openai-whisper\n# Or for GPU acceleration\npip install openai-whisper[cuda]\n"})}),"\n",(0,i.jsx)(e.h3,{id:"basic-whisper-integration",children:"Basic Whisper Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import whisper\nimport rospy\nimport pyaudio\nimport wave\nimport numpy as np\nfrom std_msgs.msg import String\n\nclass VoiceCommandProcessor:\n    def __init__(self):\n        rospy.init_node(\'voice_command_processor\')\n\n        # Load Whisper model\n        self.model = whisper.load_model("base")  # Options: tiny, base, small, medium, large\n\n        # Audio parameters\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 44100\n        self.record_seconds = 5\n\n        # Publishers and subscribers\n        self.command_pub = rospy.Publisher(\'/vla/voice_command\', String, queue_size=10)\n        self.result_pub = rospy.Publisher(\'/vla/command_result\', String, queue_size=10)\n\n        # Initialize audio stream\n        self.audio = pyaudio.PyAudio()\n\n        rospy.loginfo("Voice Command Processor initialized")\n\n    def record_audio(self):\n        """Record audio from microphone"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        rospy.loginfo("Recording...")\n        frames = []\n\n        for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\n            data = stream.read(self.chunk)\n            frames.append(data)\n\n        rospy.loginfo("Recording finished")\n\n        stream.stop_stream()\n        stream.close()\n\n        return frames\n\n    def transcribe_audio(self, frames):\n        """Transcribe recorded audio using Whisper"""\n        # Save frames to temporary WAV file\n        filename = "temp_recording.wav"\n        wf = wave.open(filename, \'wb\')\n        wf.setnchannels(self.channels)\n        wf.setsampwidth(self.audio.get_sample_size(self.format))\n        wf.setframerate(self.rate)\n        wf.writeframes(b\'\'.join(frames))\n        wf.close()\n\n        # Transcribe using Whisper\n        result = self.model.transcribe(filename)\n        transcription = result["text"]\n\n        # Clean up temporary file\n        import os\n        os.remove(filename)\n\n        return transcription.strip()\n\n    def process_voice_command(self):\n        """Main loop for processing voice commands"""\n        rate = rospy.Rate(1)  # Process commands once per second\n\n        while not rospy.is_shutdown():\n            try:\n                # Record audio\n                frames = self.record_audio()\n\n                # Transcribe to text\n                command_text = self.transcribe_audio(frames)\n\n                if command_text:\n                    rospy.loginfo(f"Recognized command: {command_text}")\n\n                    # Publish the recognized command\n                    cmd_msg = String()\n                    cmd_msg.data = command_text\n                    self.command_pub.publish(cmd_msg)\n\n                    # Process the command and execute action\n                    self.execute_command(command_text)\n\n                rate.sleep()\n\n            except Exception as e:\n                rospy.logerr(f"Error processing voice command: {str(e)}")\n\n    def execute_command(self, command_text):\n        """Parse command and execute appropriate action"""\n        command_text = command_text.lower()\n\n        if "move forward" in command_text:\n            self.move_robot("forward")\n        elif "turn left" in command_text:\n            self.move_robot("left")\n        elif "turn right" in command_text:\n            self.move_robot("right")\n        elif "stop" in command_text:\n            self.stop_robot()\n        elif "pick up" in command_text or "grasp" in command_text:\n            self.grasp_object()\n        elif "wave" in command_text:\n            self.wave_gesture()\n        else:\n            rospy.logwarn(f"Unknown command: {command_text}")\n            result_msg = String()\n            result_msg.data = f"Unknown command: {command_text}"\n            self.result_pub.publish(result_msg)\n\n    def move_robot(self, direction):\n        """Execute movement command"""\n        rospy.loginfo(f"Moving robot {direction}")\n        # Publish movement command to robot controller\n        pass\n\n    def stop_robot(self):\n        """Stop robot movement"""\n        rospy.loginfo("Stopping robot")\n        # Publish stop command\n        pass\n\n    def grasp_object(self):\n        """Execute grasping action"""\n        rospy.loginfo("Attempting to grasp object")\n        # Publish grasping command\n        pass\n\n    def wave_gesture(self):\n        """Execute waving gesture"""\n        rospy.loginfo("Waving gesture")\n        # Publish waving motion command\n        pass\n\ndef main():\n    processor = VoiceCommandProcessor()\n    try:\n        processor.process_voice_command()\n    except rospy.ROSInterruptException:\n        pass\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"natural-language-to-ros-2-action-mapping",children:"Natural Language to ROS 2 Action Mapping"}),"\n",(0,i.jsx)(e.p,{children:"Converting natural language commands to specific ROS 2 actions requires a cognitive planning system that can interpret intent and map it to executable robot behaviors."}),"\n",(0,i.jsx)(e.h3,{id:"intent-recognition-and-action-mapping",children:"Intent Recognition and Action Mapping"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import re\nimport rospy\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom actionlib_msgs.msg import GoalStatusArray\n\nclass CognitivePlanner:\n    def __init__(self):\n        rospy.init_node(\'cognitive_planner\')\n\n        # Publishers for different robot actions\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\n        self.action_pub = rospy.Publisher(\'/robot_action\', String, queue_size=10)\n\n        # Subscriber for voice commands\n        self.voice_sub = rospy.Subscriber(\'/vla/voice_command\', String, self.command_callback)\n\n        # Define command patterns and corresponding actions\n        self.command_patterns = {\n            r\'move forward|go forward|forward\': self.move_forward,\n            r\'move backward|go backward|backward\': self.move_backward,\n            r\'turn left|rotate left|left\': self.turn_left,\n            r\'turn right|rotate right|right\': self.turn_right,\n            r\'stop|halt|freeze\': self.stop_movement,\n            r\'go to (.+)|move to (.+)\': self.navigate_to_location,\n            r\'pick up (.+)|grasp (.+)|get (.+)\': self.grasp_object,\n            r\'wave|waving|hello\': self.wave_gesture,\n            r\'follow me|follow\': self.follow_person,\n        }\n\n        rospy.loginfo("Cognitive Planner initialized")\n\n    def command_callback(self, msg):\n        """Process incoming voice commands"""\n        command = msg.data.lower().strip()\n        rospy.loginfo(f"Processing command: {command}")\n\n        # Match command to pattern and execute action\n        action_executed = False\n        for pattern, action_func in self.command_patterns.items():\n            match = re.search(pattern, command)\n            if match:\n                if match.groups():  # If pattern has capture groups\n                    action_func(*match.groups())\n                else:\n                    action_func()\n                action_executed = True\n                break\n\n        if not action_executed:\n            rospy.logwarn(f"No action matched for command: {command}")\n            self.unknown_command(command)\n\n    def move_forward(self):\n        """Move robot forward"""\n        twist = Twist()\n        twist.linear.x = 0.5  # Adjust speed as needed\n        self.cmd_vel_pub.publish(twist)\n        rospy.loginfo("Moving forward")\n\n    def move_backward(self):\n        """Move robot backward"""\n        twist = Twist()\n        twist.linear.x = -0.5\n        self.cmd_vel_pub.publish(twist)\n        rospy.loginfo("Moving backward")\n\n    def turn_left(self):\n        """Turn robot left"""\n        twist = Twist()\n        twist.angular.z = 0.5\n        self.cmd_vel_pub.publish(twist)\n        rospy.loginfo("Turning left")\n\n    def turn_right(self):\n        """Turn robot right"""\n        twist = Twist()\n        twist.angular.z = -0.5\n        self.cmd_vel_pub.publish(twist)\n        rospy.loginfo("Turning right")\n\n    def stop_movement(self):\n        """Stop all robot movement"""\n        twist = Twist()\n        self.cmd_vel_pub.publish(twist)\n        rospy.loginfo("Stopping movement")\n\n    def navigate_to_location(self, location):\n        """Navigate to specified location"""\n        rospy.loginfo(f"Navigating to {location}")\n        # This would integrate with navigation stack\n        # For now, just publish the location as an action\n        action_msg = String()\n        action_msg.data = f"navigate_to:{location}"\n        self.action_pub.publish(action_msg)\n\n    def grasp_object(self, object_name):\n        """Grasp specified object"""\n        rospy.loginfo(f"Attempting to grasp {object_name}")\n        # Publish grasp command with object name\n        action_msg = String()\n        action_msg.data = f"grasp_object:{object_name}"\n        self.action_pub.publish(action_msg)\n\n    def wave_gesture(self):\n        """Perform waving gesture"""\n        rospy.loginfo("Performing waving gesture")\n        action_msg = String()\n        action_msg.data = "wave_gesture"\n        self.action_pub.publish(action_msg)\n\n    def follow_person(self):\n        """Start following person"""\n        rospy.loginfo("Starting person following")\n        action_msg = String()\n        action_msg.data = "follow_person"\n        self.action_pub.publish(action_msg)\n\n    def unknown_command(self, command):\n        """Handle unknown commands"""\n        rospy.logwarn(f"Unknown command: {command}")\n        # Could implement learning or clarification here\n\ndef main():\n    planner = CognitivePlanner()\n    rospy.spin()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"visual-scene-understanding",children:"Visual Scene Understanding"}),"\n",(0,i.jsx)(e.p,{children:"For effective VLA systems, robots must understand their visual environment to contextualize voice commands."}),"\n",(0,i.jsx)(e.h3,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import cv2\nimport numpy as np\nimport rospy\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\n\nclass VisualPerception:\n    def __init__(self):\n        rospy.init_node('visual_perception')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to camera feed\n        self.image_sub = rospy.Subscriber('/camera/rgb/image_raw', Image, self.image_callback)\n\n        # Publish object detections\n        self.detection_pub = rospy.Publisher('/vla/detections', Detection2DArray, queue_size=10)\n\n        # Load pre-trained object detection model (e.g., YOLO)\n        # For this example, we'll use a placeholder\n        self.detector = self.load_detector()\n\n        rospy.loginfo(\"Visual Perception node initialized\")\n\n    def load_detector(self):\n        \"\"\"Load object detection model\"\"\"\n        # In practice, this would load a model like YOLOv5, Detectron2, etc.\n        # For this example, we'll return a placeholder\n        return None\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera images\"\"\"\n        try:\n            # Convert ROS image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Perform object detection\n            detections = self.detect_objects(cv_image)\n\n            # Publish detections\n            self.publish_detections(detections, msg.header)\n\n        except Exception as e:\n            rospy.logerr(f\"Error processing image: {str(e)}\")\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects in the image\"\"\"\n        # Placeholder for object detection\n        # In practice, this would run inference on a deep learning model\n        height, width = image.shape[:2]\n\n        # Example detections (in practice, these would come from the model)\n        detections = [\n            {\n                'class': 'person',\n                'confidence': 0.95,\n                'bbox': [int(width * 0.3), int(height * 0.4), int(width * 0.2), int(height * 0.4)]\n            },\n            {\n                'class': 'chair',\n                'confidence': 0.87,\n                'bbox': [int(width * 0.6), int(height * 0.5), int(width * 0.2), int(height * 0.3)]\n            }\n        ]\n\n        return detections\n\n    def publish_detections(self, detections, header):\n        \"\"\"Publish object detections as ROS messages\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for det in detections:\n            detection = Detection2D()\n            detection.header = header\n\n            # Set bounding box\n            detection.bbox.size_x = det['bbox'][2]\n            detection.bbox.size_y = det['bbox'][3]\n            detection.bbox.center.x = det['bbox'][0] + det['bbox'][2] / 2\n            detection.bbox.center.y = det['bbox'][1] + det['bbox'][3] / 2\n\n            # Set classification\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.id = det['class']\n            hypothesis.score = det['confidence']\n            detection.results.append(hypothesis)\n\n            detection_array.detections.append(detection)\n\n        self.detection_pub.publish(detection_array)\n\ndef main():\n    perception = VisualPerception()\n    rospy.spin()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"voice-command-pipeline-integration",children:"Voice Command Pipeline Integration"}),"\n",(0,i.jsx)(e.p,{children:"Combining all components into a complete VLA system:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rospy\nimport threading\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\n\nclass VLASystem:\n    def __init__(self):\n        rospy.init_node(\'vla_system\')\n\n        # Publishers\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\n        self.status_pub = rospy.Publisher(\'/vla/status\', String, queue_size=10)\n\n        # Subscribers\n        self.voice_sub = rospy.Subscriber(\'/vla/voice_command\', String, self.voice_command_callback)\n        self.vision_sub = rospy.Subscriber(\'/vla/detections\', String, self.vision_callback)\n\n        # System state\n        self.current_context = {}\n        self.is_listening = True\n\n        rospy.loginfo("VLA System initialized")\n\n    def voice_command_callback(self, msg):\n        """Process voice command in context of visual information"""\n        command = msg.data\n\n        # Combine voice command with visual context\n        action = self.interpret_command_with_context(command, self.current_context)\n\n        if action:\n            self.execute_action(action)\n            self.update_status(f"Executed: {action}")\n\n    def vision_callback(self, msg):\n        """Update visual context"""\n        # Update current visual context based on detections\n        self.current_context[\'objects\'] = msg.data  # Simplified\n        rospy.loginfo(f"Updated visual context: {msg.data}")\n\n    def interpret_command_with_context(self, command, context):\n        """Interpret command using visual context"""\n        command_lower = command.lower()\n\n        # Example: "Pick up the red ball" - need to find red ball in context\n        if "pick up" in command_lower or "grasp" in command_lower:\n            # Look for objects that match the description in visual context\n            if "red ball" in command_lower and "red ball" in context.get(\'objects\', \'\'):\n                return "grasp_red_ball"\n\n        elif "go to" in command_lower:\n            # Use navigation based on visual landmarks\n            return f"navigate_to:{command_lower.replace(\'go to\', \'\').strip()}"\n\n        # Default interpretation\n        return command\n\n    def execute_action(self, action):\n        """Execute the interpreted action"""\n        rospy.loginfo(f"Executing action: {action}")\n\n        # Map action to ROS command\n        if "forward" in action:\n            self.move_forward()\n        elif "grasp" in action:\n            self.execute_grasp()\n        elif "navigate" in action:\n            self.start_navigation(action)\n        # Add more action mappings as needed\n\n    def move_forward(self):\n        """Move robot forward"""\n        twist = Twist()\n        twist.linear.x = 0.3\n        self.cmd_vel_pub.publish(twist)\n\n    def execute_grasp(self):\n        """Execute grasping action (placeholder)"""\n        rospy.loginfo("Executing grasp action")\n        # Publish grasp command to manipulation system\n\n    def start_navigation(self, action):\n        """Start navigation to specified location"""\n        rospy.loginfo(f"Starting navigation: {action}")\n        # Integrate with navigation stack\n\n    def update_status(self, status):\n        """Publish system status"""\n        status_msg = String()\n        status_msg.data = status\n        self.status_pub.publish(status_msg)\n\ndef main():\n    vla_system = VLASystem()\n\n    # Keep the node running\n    try:\n        rospy.spin()\n    except KeyboardInterrupt:\n        rospy.loginfo("VLA System shutting down")\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"integration-with-humanoid-robot-control",children:"Integration with Humanoid Robot Control"}),"\n",(0,i.jsx)(e.p,{children:"Connecting VLA systems to humanoid robot actuators requires careful consideration of the robot's kinematic structure and safety constraints."}),"\n",(0,i.jsx)(e.h3,{id:"humanoid-action-execution",children:"Humanoid Action Execution"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rospy\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryGoal\nimport actionlib\n\nclass HumanoidActionExecutor:\n    def __init__(self):\n        rospy.init_node(\'humanoid_action_executor\')\n\n        # Joint trajectory publisher for simple movements\n        self.joint_pub = rospy.Publisher(\'/joint_trajectory_controller/command\', JointTrajectory, queue_size=10)\n\n        # Action client for complex movements\n        self.trajectory_client = actionlib.SimpleActionClient(\n            \'/joint_trajectory_controller/follow_joint_trajectory\',\n            FollowJointTrajectoryAction\n        )\n\n        # Wait for action server\n        rospy.loginfo("Waiting for joint trajectory controller...")\n        self.trajectory_client.wait_for_server()\n        rospy.loginfo("Connected to joint trajectory controller")\n\n        # Define joint names for humanoid robot\n        self.joint_names = [\n            \'left_hip_joint\', \'left_knee_joint\', \'left_ankle_joint\',\n            \'right_hip_joint\', \'right_knee_joint\', \'right_ankle_joint\',\n            \'left_shoulder_joint\', \'left_elbow_joint\', \'left_wrist_joint\',\n            \'right_shoulder_joint\', \'right_elbow_joint\', \'right_wrist_joint\'\n        ]\n\n    def execute_waving_motion(self):\n        """Execute waving gesture with arm joints"""\n        trajectory = JointTrajectory()\n        trajectory.joint_names = self.joint_names\n\n        # Create trajectory points for waving motion\n        point1 = JointTrajectoryPoint()\n        point1.positions = [0.0] * len(self.joint_names)  # Default positions\n        point1.positions[7] = 1.0  # Left elbow up\n        point1.time_from_start = rospy.Duration(1.0)\n\n        point2 = JointTrajectoryPoint()\n        point2.positions = point1.positions[:]\n        point2.positions[7] = -1.0  # Left elbow down\n        point2.time_from_start = rospy.Duration(2.0)\n\n        trajectory.points = [point1, point2]\n        trajectory.header.stamp = rospy.Time.now()\n\n        self.joint_pub.publish(trajectory)\n\n    def execute_walking_gait(self):\n        """Execute basic walking gait"""\n        trajectory = JointTrajectory()\n        trajectory.joint_names = self.joint_names\n\n        # Simplified walking gait (in practice, this would be much more complex)\n        points = []\n        time_step = 0.5\n\n        for i in range(10):  # 10 steps\n            point = JointTrajectoryPoint()\n            point.positions = [0.0] * len(self.joint_names)\n\n            # Alternate leg movements for walking\n            if i % 2 == 0:\n                point.positions[0] = 0.2  # Left hip forward\n                point.positions[3] = -0.1  # Right hip back\n            else:\n                point.positions[0] = -0.1  # Left hip back\n                point.positions[3] = 0.2  # Right hip forward\n\n            point.time_from_start = rospy.Duration(i * time_step)\n            points.append(point)\n\n        trajectory.points = points\n        trajectory.header.stamp = rospy.Time.now()\n\n        self.joint_pub.publish(trajectory)\n\n    def execute_voice_command(self, command):\n        """Map voice command to humanoid action"""\n        command_lower = command.lower()\n\n        if "wave" in command_lower:\n            self.execute_waving_motion()\n        elif "walk" in command_lower or "move" in command_lower:\n            self.execute_walking_gait()\n        elif "stand" in command_lower:\n            self.stand_up()\n        # Add more commands as needed\n\n    def stand_up(self):\n        """Return to standing position"""\n        trajectory = JointTrajectory()\n        trajectory.joint_names = self.joint_names\n\n        point = JointTrajectoryPoint()\n        point.positions = [0.0] * len(self.joint_names)  # Neutral standing position\n        point.time_from_start = rospy.Duration(2.0)\n\n        trajectory.points = [point]\n        trajectory.header.stamp = rospy.Time.now()\n\n        self.joint_pub.publish(trajectory)\n\ndef main():\n    executor = HumanoidActionExecutor()\n\n    # Example: Execute waving motion when node starts\n    rospy.sleep(1.0)  # Wait for publishers to connect\n    executor.execute_waving_motion()\n\n    rospy.spin()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"practice-tasks",children:"Practice Tasks"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Install and configure OpenAI Whisper for voice recognition"}),"\n",(0,i.jsx)(e.li,{children:"Create a simple voice command that makes your robot move forward"}),"\n",(0,i.jsx)(e.li,{children:"Implement object detection that identifies basic objects in the camera feed"}),"\n",(0,i.jsx)(e.li,{children:'Develop a system that combines voice commands with visual information (e.g., "pick up the red ball")'}),"\n",(0,i.jsx)(e.li,{children:'Create a gesture that your humanoid robot performs when it hears "hello"'}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"In this chapter, you've learned to build Vision-Language-Action systems that enable natural human-robot interaction:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"How to integrate OpenAI Whisper for voice command recognition"}),"\n",(0,i.jsx)(e.li,{children:"Techniques for mapping natural language to ROS 2 actions"}),"\n",(0,i.jsx)(e.li,{children:"Methods for incorporating visual scene understanding"}),"\n",(0,i.jsx)(e.li,{children:"Approaches for combining voice, vision, and action in a unified system"}),"\n",(0,i.jsx)(e.li,{children:"Implementation of humanoid-specific action execution"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"VLA systems represent the future of human-robot interaction, making robots more accessible and intuitive to use. By combining speech recognition, visual perception, and intelligent action planning, you can create humanoid robots that respond naturally to human commands and adapt to their environment."})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(m,{...n})}):m(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>r});var o=t(6540);const i={},s=o.createContext(i);function a(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);