"use strict";(globalThis.webpackChunkmy_book_site=globalThis.webpackChunkmy_book_site||[]).push([[856],{7499:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"chapter7-manipulation","title":"Chapter 7: Manipulation and Grasping for Humanoid Robots","description":"Introduction to Manipulation in Humanoid Robots","source":"@site/docs/chapter7-manipulation.md","sourceDirName":".","slug":"/chapter7-manipulation","permalink":"/my_book/docs/chapter7-manipulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Ayesha788/my_book/tree/main/docs/chapter7-manipulation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6: Computer Vision for Humanoid Robotics","permalink":"/my_book/docs/chapter6-computer-vision"},"next":{"title":"Chapter 8: Multi-Robot Coordination for Humanoid Systems","permalink":"/my_book/docs/chapter8-multirobot"}}');var o=t(4848),a=t(8453);const r={},s="Chapter 7: Manipulation and Grasping for Humanoid Robots",l={},p=[{value:"Introduction to Manipulation in Humanoid Robots",id:"introduction-to-manipulation-in-humanoid-robots",level:2},{value:"Kinematics for Humanoid Arms",id:"kinematics-for-humanoid-arms",level:2},{value:"Forward and Inverse Kinematics",id:"forward-and-inverse-kinematics",level:3},{value:"Grasp Planning and Execution",id:"grasp-planning-and-execution",level:2},{value:"Grasp Pose Generation",id:"grasp-pose-generation",level:3},{value:"Multi-Fingered Hand Control",id:"multi-fingered-hand-control",level:2},{value:"Grasp Force Optimization",id:"grasp-force-optimization",level:3},{value:"Grasp Stability and Learning",id:"grasp-stability-and-learning",level:2},{value:"Grasp Quality Metrics",id:"grasp-quality-metrics",level:3},{value:"Integration with Robot Control",id:"integration-with-robot-control",level:2},{value:"ROS 2 Manipulation Interface",id:"ros-2-manipulation-interface",level:3},{value:"Grasp Learning and Adaptation",id:"grasp-learning-and-adaptation",level:2},{value:"Reinforcement Learning for Grasp Improvement",id:"reinforcement-learning-for-grasp-improvement",level:3},{value:"Challenges in Humanoid Manipulation",id:"challenges-in-humanoid-manipulation",level:2},{value:"Balance-Grasp Coordination",id:"balance-grasp-coordination",level:3},{value:"Real-time Grasp Planning",id:"real-time-grasp-planning",level:3},{value:"Multi-modal Sensing",id:"multi-modal-sensing",level:3},{value:"Practice Tasks",id:"practice-tasks",level:2},{value:"Summary",id:"summary",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-7-manipulation-and-grasping-for-humanoid-robots",children:"Chapter 7: Manipulation and Grasping for Humanoid Robots"})}),"\n",(0,o.jsx)(e.h2,{id:"introduction-to-manipulation-in-humanoid-robots",children:"Introduction to Manipulation in Humanoid Robots"}),"\n",(0,o.jsx)(e.p,{children:"Manipulation is a fundamental capability for humanoid robots, enabling them to interact with objects in their environment. Unlike specialized manipulators, humanoid robots must coordinate manipulation tasks with balance and locomotion, making it a complex multi-constraint problem."}),"\n",(0,o.jsx)(e.h2,{id:"kinematics-for-humanoid-arms",children:"Kinematics for Humanoid Arms"}),"\n",(0,o.jsx)(e.h3,{id:"forward-and-inverse-kinematics",children:"Forward and Inverse Kinematics"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots typically have 7-DOF arms similar to human arms, requiring sophisticated kinematic solutions:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass HumanoidArmKinematics:\n    def __init__(self, joint_limits=None):\n        # DH parameters for a typical humanoid arm\n        # [a, alpha, d, theta_offset]\n        self.dh_params = [\n            [0, np.pi/2, 0, 0],           # Shoulder joint 1\n            [0, -np.pi/2, 0, np.pi/2],    # Shoulder joint 2\n            [0, np.pi/2, 0, 0],           # Shoulder joint 3\n            [0, -np.pi/2, 0.3, 0],        # Elbow joint\n            [0, np.pi/2, 0, 0],           # Wrist joint 1\n            [0, -np.pi/2, 0.25, 0],       # Wrist joint 2\n            [0, 0, 0.05, 0]               # Wrist joint 3\n        ]\n\n        self.joint_limits = joint_limits or [\n            [-2.0, 2.0],    # Shoulder 1\n            [-2.0, 1.5],    # Shoulder 2\n            [-2.0, 2.0],    # Shoulder 3\n            [-3.0, 0.5],    # Elbow\n            [-2.0, 2.0],    # Wrist 1\n            [-1.0, 1.0],    # Wrist 2\n            [-2.0, 2.0]     # Wrist 3\n        ]\n\n    def dh_transform(self, a, alpha, d, theta):\n        """Compute DH transformation matrix"""\n        return np.array([\n            [np.cos(theta), -np.sin(theta)*np.cos(alpha), np.sin(theta)*np.sin(alpha), a*np.cos(theta)],\n            [np.sin(theta), np.cos(theta)*np.cos(alpha), -np.cos(theta)*np.sin(alpha), a*np.sin(theta)],\n            [0, np.sin(alpha), np.cos(alpha), d],\n            [0, 0, 0, 1]\n        ])\n\n    def forward_kinematics(self, joint_angles):\n        """Compute end-effector pose from joint angles"""\n        T = np.eye(4)\n\n        for i, (a, alpha, d, theta_offset) in enumerate(self.dh_params):\n            theta = joint_angles[i] + theta_offset\n            T_link = self.dh_transform(a, alpha, d, theta)\n            T = T @ T_link\n\n        return T\n\n    def inverse_kinematics(self, target_pose, current_joints, max_iterations=100, tolerance=1e-4):\n        """Solve inverse kinematics using Jacobian transpose method"""\n        joints = current_joints.copy()\n\n        for iteration in range(max_iterations):\n            # Compute current end-effector pose\n            current_pose = self.forward_kinematics(joints)\n\n            # Compute error\n            pos_error = target_pose[:3, 3] - current_pose[:3, 3]\n            rot_error = R.from_matrix(\n                current_pose[:3, :3].T @ target_pose[:3, :3]\n            ).as_rotvec()\n\n            error = np.concatenate([pos_error, rot_error])\n\n            if np.linalg.norm(error) < tolerance:\n                break\n\n            # Compute Jacobian\n            J = self.compute_jacobian(joints)\n\n            # Update joints using Jacobian transpose\n            joint_delta = 0.1 * J.T @ error\n            joints += joint_delta\n\n            # Apply joint limits\n            for i in range(len(joints)):\n                joints[i] = np.clip(joints[i], self.joint_limits[i][0], self.joint_limits[i][1])\n\n        return joints\n\n    def compute_jacobian(self, joint_angles):\n        """Compute geometric Jacobian"""\n        J = np.zeros((6, len(joint_angles)))\n\n        current_transform = np.eye(4)\n        end_effector_pose = self.forward_kinematics(joint_angles)\n\n        for i, (a, alpha, d, theta_offset) in enumerate(self.dh_params):\n            theta = joint_angles[i] + theta_offset\n            T_link = self.dh_transform(a, alpha, d, theta)\n            current_transform = current_transform @ T_link\n\n            # Z-axis of current joint\n            z_axis = current_transform[:3, 2]\n            # Position from current joint to end effector\n            r = end_effector_pose[:3, 3] - current_transform[:3, 3]\n\n            # Linear velocity component\n            J[:3, i] = np.cross(z_axis, r)\n            # Angular velocity component\n            J[3:, i] = z_axis\n\n        return J\n'})}),"\n",(0,o.jsx)(e.h2,{id:"grasp-planning-and-execution",children:"Grasp Planning and Execution"}),"\n",(0,o.jsx)(e.h3,{id:"grasp-pose-generation",children:"Grasp Pose Generation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import open3d as o3d\nimport numpy as np\n\nclass GraspPlanner:\n    def __init__(self):\n        self.approach_distance = 0.1  # Distance to approach object\n        self.grasp_width_range = [0.02, 0.15]  # Min/max grasp width\n\n    def generate_grasp_poses(self, object_mesh, approach_directions=None):\n        """Generate potential grasp poses for an object"""\n        if approach_directions is None:\n            # Default approach directions (front, side, top)\n            approach_directions = [\n                [1, 0, 0],   # Front approach\n                [-1, 0, 0],  # Back approach\n                [0, 1, 0],   # Side approach\n                [0, -1, 0],  # Opposite side\n                [0, 0, 1]    # Top approach\n            ]\n\n        grasp_poses = []\n\n        # Sample points on the object surface\n        surface_points = self.sample_surface_points(object_mesh)\n\n        for point in surface_points:\n            for approach_dir in approach_directions:\n                # Generate grasp pose\n                grasp_pose = self.create_grasp_pose(point, approach_dir)\n\n                # Check if grasp is feasible\n                if self.is_grasp_feasible(grasp_pose, object_mesh):\n                    grasp_poses.append(grasp_pose)\n\n        return grasp_poses\n\n    def sample_surface_points(self, mesh, num_points=100):\n        """Sample points on the mesh surface"""\n        pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n        return np.asarray(pcd.points)\n\n    def create_grasp_pose(self, contact_point, approach_direction):\n        """Create a grasp pose from contact point and approach direction"""\n        # Normalize approach direction\n        approach = np.array(approach_direction) / np.linalg.norm(approach_direction)\n\n        # Create orthogonal axes for the grasp frame\n        # For now, assume a simple orientation\n        z_axis = -approach  # Grasp direction (opposite to approach)\n\n        # Create an arbitrary orthogonal x-axis\n        if abs(z_axis[2]) < 0.9:\n            x_axis = np.cross(z_axis, [0, 0, 1])\n        else:\n            x_axis = np.cross(z_axis, [1, 0, 0])\n        x_axis = x_axis / np.linalg.norm(x_axis)\n\n        y_axis = np.cross(z_axis, x_axis)\n\n        # Create rotation matrix\n        R_grasp = np.column_stack([x_axis, y_axis, z_axis])\n\n        # Position is the contact point moved back by approach distance\n        position = contact_point - approach * self.approach_distance\n\n        # Create transformation matrix\n        T = np.eye(4)\n        T[:3, :3] = R_grasp\n        T[:3, 3] = position\n\n        return T\n\n    def is_grasp_feasible(self, grasp_pose, object_mesh):\n        """Check if a grasp pose is geometrically feasible"""\n        # Check grasp width constraints\n        # Check collision with environment\n        # Check accessibility of the robot\n        return True  # Simplified for example\n'})}),"\n",(0,o.jsx)(e.h2,{id:"multi-fingered-hand-control",children:"Multi-Fingered Hand Control"}),"\n",(0,o.jsx)(e.h3,{id:"grasp-force-optimization",children:"Grasp Force Optimization"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class MultiFingeredHandController:\n    def __init__(self, num_fingers=5):\n        self.num_fingers = num_fingers\n        self.finger_positions = np.zeros(num_fingers)  # Joint positions\n        self.finger_forces = np.zeros(num_fingers)     # Applied forces\n\n    def compute_grasp_forces(self, object_weight, object_com, contact_points):\n        """\n        Compute optimal finger forces to grasp an object\n        """\n        # Object properties\n        gravity_force = np.array([0, 0, -object_weight * 9.81])\n\n        # Set up force equilibrium equations\n        # Sum of forces = 0\n        # Sum of torques = 0\n\n        # For each contact point, we have friction constraints\n        # Force must be within friction cone\n\n        # Simplified: distribute weight equally among contact points\n        num_contacts = len(contact_points)\n        if num_contacts > 0:\n            force_per_contact = object_weight * 9.81 / num_contacts\n            return np.full(num_contacts, force_per_contact)\n        else:\n            return np.array([])\n\n    def grasp_object(self, grasp_pose, object_properties):\n        """Execute a grasp with optimal forces"""\n        # Move to pre-grasp position\n        pre_grasp_pose = grasp_pose.copy()\n        pre_grasp_pose[2, 3] += 0.05  # Move 5cm above grasp point\n\n        # Execute pre-grasp motion\n        self.move_to_pose(pre_grasp_pose)\n\n        # Move to grasp position\n        self.move_to_pose(grasp_pose)\n\n        # Compute and apply grasp forces\n        grasp_forces = self.compute_grasp_forces(\n            object_properties[\'weight\'],\n            object_properties[\'center_of_mass\'],\n            object_properties[\'contact_points\']\n        )\n\n        self.apply_forces(grasp_forces)\n\n        # Lift object\n        lift_pose = grasp_pose.copy()\n        lift_pose[2, 3] += 0.1  # Lift 10cm\n        self.move_to_pose(lift_pose)\n\n    def move_to_pose(self, pose):\n        """Move hand to specified pose"""\n        # Implementation depends on robot hardware\n        pass\n\n    def apply_forces(self, forces):\n        """Apply specified forces to fingers"""\n        # Implementation depends on hand design\n        pass\n'})}),"\n",(0,o.jsx)(e.h2,{id:"grasp-stability-and-learning",children:"Grasp Stability and Learning"}),"\n",(0,o.jsx)(e.h3,{id:"grasp-quality-metrics",children:"Grasp Quality Metrics"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class GraspQualityEvaluator:\n    def __init__(self):\n        pass\n\n    def evaluate_grasp_quality(self, grasp_pose, object_mesh, contact_points, friction_coeff=0.8):\n        """\n        Evaluate the quality of a grasp based on multiple metrics\n        """\n        quality_metrics = {}\n\n        # 1. Force closure (ability to resist arbitrary wrenches)\n        quality_metrics[\'force_closure\'] = self.check_force_closure(contact_points, friction_coeff)\n\n        # 2. Grasp isotropy (uniform resistance in all directions)\n        quality_metrics[\'isotropy\'] = self.compute_isotropy_index(contact_points)\n\n        # 3. Volume of the grasp wrench space\n        quality_metrics[\'wrench_space_volume\'] = self.compute_wrench_space_volume(\n            contact_points, friction_coeff\n        )\n\n        # 4. Resistance to object weight\n        quality_metrics[\'weight_resistance\'] = self.compute_weight_resistance(\n            grasp_pose, contact_points\n        )\n\n        # Overall quality score (weighted combination)\n        weights = {\n            \'force_closure\': 0.3,\n            \'isotropy\': 0.2,\n            \'wrench_space_volume\': 0.3,\n            \'weight_resistance\': 0.2\n        }\n\n        overall_quality = sum(\n            weights[key] * value for key, value in quality_metrics.items()\n        )\n\n        quality_metrics[\'overall\'] = overall_quality\n\n        return quality_metrics\n\n    def check_force_closure(self, contact_points, friction_coeff):\n        """\n        Check if the grasp provides force closure\n        """\n        # For 3D grasping, we need at least 7 contact points for force closure\n        # Or specific arrangements of fewer points\n        if len(contact_points) >= 7:\n            return 1.0  # High probability of force closure\n        elif len(contact_points) >= 3:\n            # Check specific geometric arrangements\n            return 0.7\n        else:\n            return 0.3  # Low probability of force closure\n\n    def compute_isotropy_index(self, contact_points):\n        """\n        Compute how uniformly the grasp resists forces in different directions\n        """\n        # Calculate the grasp matrix and its condition number\n        # Lower condition number indicates better isotropy\n        if len(contact_points) < 3:\n            return 0.0\n\n        # Simplified isotropy calculation\n        # In practice, this would involve the grasp matrix\n        return 0.6  # Placeholder value\n\n    def compute_wrench_space_volume(self, contact_points, friction_coeff):\n        """\n        Compute the volume of the wrench space that can be resisted\n        """\n        # This is a complex calculation involving the grasp matrix\n        # and friction cones\n        return 0.8  # Placeholder value\n\n    def compute_weight_resistance(self, grasp_pose, contact_points):\n        """\n        Compute how well the grasp resists the object\'s weight\n        """\n        # Calculate if the grasp can resist gravitational forces\n        # based on contact positions and friction\n        return 0.9  # Placeholder value\n'})}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-robot-control",children:"Integration with Robot Control"}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-manipulation-interface",children:"ROS 2 Manipulation Interface"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import String\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\n\nclass ManipulationController(Node):\n    def __init__(self):\n        super().__init__('manipulation_controller')\n\n        # Publishers and subscribers\n        self.joint_cmd_pub = self.create_publisher(\n            JointTrajectory,\n            '/arm_controller/joint_trajectory',\n            10\n        )\n\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        self.grasp_cmd_sub = self.create_subscription(\n            PoseStamped,\n            '/grasp_target',\n            self.grasp_callback,\n            10\n        )\n\n        # Internal state\n        self.current_joints = None\n        self.arm_kinematics = HumanoidArmKinematics()\n        self.grasp_planner = GraspPlanner()\n\n        self.get_logger().info('Manipulation controller initialized')\n\n    def joint_state_callback(self, msg):\n        \"\"\"Update current joint positions\"\"\"\n        self.current_joints = np.array(msg.position[-7:])  # Last 7 joints for arm\n\n    def grasp_callback(self, msg):\n        \"\"\"Handle grasp command\"\"\"\n        if self.current_joints is None:\n            self.get_logger().warn('No joint state available')\n            return\n\n        # Convert target pose to transformation matrix\n        target_pose = self.pose_to_matrix(msg.pose)\n\n        # Solve inverse kinematics\n        joint_goals = self.arm_kinematics.inverse_kinematics(\n            target_pose,\n            self.current_joints\n        )\n\n        # Publish joint trajectory\n        self.execute_trajectory(joint_goals)\n\n    def pose_to_matrix(self, pose):\n        \"\"\"Convert ROS Pose to 4x4 transformation matrix\"\"\"\n        matrix = np.eye(4)\n\n        # Position\n        matrix[0, 3] = pose.position.x\n        matrix[1, 3] = pose.position.y\n        matrix[2, 3] = pose.position.z\n\n        # Orientation (convert quaternion to rotation matrix)\n        import tf_transformations as tf\n        q = [pose.orientation.x, pose.orientation.y,\n             pose.orientation.z, pose.orientation.w]\n        matrix[:3, :3] = tf.quaternion_matrix(q)[:3, :3]\n\n        return matrix\n\n    def execute_trajectory(self, joint_goals):\n        \"\"\"Execute joint trajectory to reach target\"\"\"\n        trajectory = JointTrajectory()\n        trajectory.joint_names = [\n            'shoulder_pitch', 'shoulder_roll', 'shoulder_yaw',\n            'elbow_pitch', 'wrist_pitch', 'wrist_yaw', 'wrist_roll'\n        ]\n\n        point = JointTrajectoryPoint()\n        point.positions = joint_goals.tolist()\n        point.time_from_start.sec = 2  # 2 seconds to reach goal\n        point.time_from_start.nanosec = 0\n\n        trajectory.points = [point]\n\n        self.joint_cmd_pub.publish(trajectory)\n"})}),"\n",(0,o.jsx)(e.h2,{id:"grasp-learning-and-adaptation",children:"Grasp Learning and Adaptation"}),"\n",(0,o.jsx)(e.h3,{id:"reinforcement-learning-for-grasp-improvement",children:"Reinforcement Learning for Grasp Improvement"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass GraspPolicyNetwork(nn.Module):\n    def __init__(self, state_dim=20, action_dim=7):\n        super(GraspPolicyNetwork, self).__init__()\n\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_dim),\n            nn.Tanh()  # Output actions between -1 and 1\n        )\n\n    def forward(self, state):\n        return self.network(state)\n\nclass GraspLearner:\n    def __init__(self):\n        self.policy_network = GraspPolicyNetwork()\n        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=0.001)\n        self.replay_buffer = []\n\n    def get_grasp_action(self, state):\n        """Get grasp action from current policy"""\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        action = self.policy_network(state_tensor)\n        return action.squeeze(0).detach().numpy()\n\n    def update_policy(self, states, actions, rewards, next_states):\n        """Update policy using collected experiences"""\n        states = torch.FloatTensor(states)\n        actions = torch.FloatTensor(actions)\n        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n\n        # Compute predicted Q values\n        predicted_actions = self.policy_network(states)\n\n        # Compute loss (negative reward for bad grasps, positive for good)\n        loss = -torch.mean(rewards * torch.sum((actions - predicted_actions)**2, dim=1))\n\n        # Update network\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        return loss.item()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"challenges-in-humanoid-manipulation",children:"Challenges in Humanoid Manipulation"}),"\n",(0,o.jsx)(e.h3,{id:"balance-grasp-coordination",children:"Balance-Grasp Coordination"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots must coordinate manipulation with balance:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Whole-body control that considers both manipulation and balance"}),"\n",(0,o.jsx)(e.li,{children:"Dynamic weight shifting during manipulation tasks"}),"\n",(0,o.jsx)(e.li,{children:"Recovery strategies when manipulation affects balance"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"real-time-grasp-planning",children:"Real-time Grasp Planning"}),"\n",(0,o.jsx)(e.p,{children:"Challenges in real-time grasp planning:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Fast collision checking"}),"\n",(0,o.jsx)(e.li,{children:"Efficient grasp quality evaluation"}),"\n",(0,o.jsx)(e.li,{children:"Handling dynamic environments"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"multi-modal-sensing",children:"Multi-modal Sensing"}),"\n",(0,o.jsx)(e.p,{children:"Integrating different sensory modalities:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Vision-based grasp planning"}),"\n",(0,o.jsx)(e.li,{children:"Tactile feedback during grasp execution"}),"\n",(0,o.jsx)(e.li,{children:"Force control for safe interaction"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"practice-tasks",children:"Practice Tasks"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement inverse kinematics for a humanoid arm using multiple methods"}),"\n",(0,o.jsx)(e.li,{children:"Create a grasp planner that works with point cloud data"}),"\n",(0,o.jsx)(e.li,{children:"Develop a grasp quality evaluator for different object shapes"}),"\n",(0,o.jsx)(e.li,{children:"Integrate manipulation with balance control in simulation"}),"\n",(0,o.jsx)(e.li,{children:"Test grasping algorithms on various object types and sizes"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"Manipulation and grasping are essential capabilities for humanoid robots to interact with their environment. By combining kinematic solutions, grasp planning algorithms, and learning techniques, humanoid robots can perform complex manipulation tasks while maintaining balance and safety."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(c,{...n})}):c(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>s});var i=t(6540);const o={},a=i.createContext(o);function r(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);