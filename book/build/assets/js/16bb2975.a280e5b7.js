"use strict";(globalThis.webpackChunkmy_book_site=globalThis.webpackChunkmy_book_site||[]).push([[73],{5839:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"chapter6-computer-vision","title":"Chapter 6: Computer Vision for Humanoid Robotics","description":"Introduction to Computer Vision in Robotics","source":"@site/docs/chapter6-computer-vision.md","sourceDirName":".","slug":"/chapter6-computer-vision","permalink":"/my_book/docs/chapter6-computer-vision","draft":false,"unlisted":false,"editUrl":"https://github.com/Ayesha788/my_book/tree/main/docs/chapter6-computer-vision.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Path Planning and Navigation for Humanoid Robots","permalink":"/my_book/docs/chapter5-path-planning"},"next":{"title":"Chapter 7: Manipulation and Grasping for Humanoid Robots","permalink":"/my_book/docs/chapter7-manipulation"}}');var t=i(4848),s=i(8453);const r={},a="Chapter 6: Computer Vision for Humanoid Robotics",c={},l=[{value:"Introduction to Computer Vision in Robotics",id:"introduction-to-computer-vision-in-robotics",level:2},{value:"Vision Systems for Humanoid Robots",id:"vision-systems-for-humanoid-robots",level:2},{value:"Stereo Vision Setup",id:"stereo-vision-setup",level:3},{value:"RGB-D Integration",id:"rgb-d-integration",level:3},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:2},{value:"Deep Learning for Object Detection",id:"deep-learning-for-object-detection",level:3},{value:"Visual SLAM for Humanoid Robots",id:"visual-slam-for-humanoid-robots",level:2},{value:"Face Detection and Recognition",id:"face-detection-and-recognition",level:2},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Image Processing Node",id:"image-processing-node",level:3},{value:"Challenges in Humanoid Robot Vision",id:"challenges-in-humanoid-robot-vision",level:2},{value:"Real-time Processing",id:"real-time-processing",level:3},{value:"Varying Lighting Conditions",id:"varying-lighting-conditions",level:3},{value:"Motion Artifacts",id:"motion-artifacts",level:3},{value:"Practice Tasks",id:"practice-tasks",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-6-computer-vision-for-humanoid-robotics",children:"Chapter 6: Computer Vision for Humanoid Robotics"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-computer-vision-in-robotics",children:"Introduction to Computer Vision in Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Computer vision is essential for humanoid robots to perceive and understand their environment. Unlike traditional robots, humanoid robots have multiple sensors positioned similar to human vision systems, enabling them to process visual information in ways that mimic human perception."}),"\n",(0,t.jsx)(n.h2,{id:"vision-systems-for-humanoid-robots",children:"Vision Systems for Humanoid Robots"}),"\n",(0,t.jsx)(n.h3,{id:"stereo-vision-setup",children:"Stereo Vision Setup"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots typically employ stereo vision systems to perceive depth, similar to human binocular vision:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\nclass StereoVisionSystem:\n    def __init__(self, left_camera_params, right_camera_params):\n        # Camera matrices and distortion coefficients\n        self.left_K = left_camera_params['K']\n        self.left_dist = left_camera_params['dist']\n        self.right_K = right_camera_params['K']\n        self.right_dist = right_camera_params['dist']\n\n        # Rectification parameters\n        self.R1, self.R2, self.P1, self.P2, self.Q = self.compute_rectification()\n\n    def compute_rectification(self):\n        # Compute rectification transforms\n        R1, R2, P1, P2, Q, _, _ = cv2.stereoRectify(\n            self.left_K, self.left_dist,\n            self.right_K, self.right_dist,\n            (640, 480),  # Image size\n            None, None    # Rotation and translation between cameras\n        )\n        return R1, R2, P1, P2, Q\n\n    def compute_disparity(self, left_img, right_img):\n        # Rectify images\n        left_rectified = cv2.remap(left_img, self.left_map1, self.left_map2, cv2.INTER_LINEAR)\n        right_rectified = cv2.remap(right_img, self.right_map1, self.right_map2, cv2.INTER_LINEAR)\n\n        # Compute disparity\n        stereo = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=16*10,  # Must be divisible by 16\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n        disparity = stereo.compute(left_rectified, right_rectified).astype(np.float32) / 16.0\n        return disparity\n\n    def depth_from_disparity(self, disparity):\n        # Convert disparity to depth using Q matrix\n        points = cv2.reprojectImageTo3D(disparity, self.Q)\n        return points\n"})}),"\n",(0,t.jsx)(n.h3,{id:"rgb-d-integration",children:"RGB-D Integration"}),"\n",(0,t.jsx)(n.p,{children:"RGB-D sensors provide both color and depth information, which is crucial for humanoid robot perception:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import open3d as o3d\nimport numpy as np\n\nclass RGBDProcessor:\n    def __init__(self, camera_intrinsics):\n        self.intrinsics = camera_intrinsics\n        self.fov_x = camera_intrinsics['fov_x']\n        self.fov_y = camera_intrinsics['fov_y']\n\n    def create_point_cloud(self, rgb_image, depth_image, depth_scale=1000.0):\n        # Convert to Open3D format\n        rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n            o3d.geometry.Image(rgb_image),\n            o3d.geometry.Image(depth_image),\n            depth_scale=depth_scale,\n            depth_trunc=3.0,\n            convert_rgb_to_intensity=False\n        )\n\n        # Create point cloud\n        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\n            rgbd_image,\n            o3d.camera.PinholeCameraIntrinsic(\n                width=rgb_image.shape[1],\n                height=rgb_image.shape[0],\n                fx=self.intrinsics['fx'],\n                fy=self.intrinsics['fy'],\n                cx=self.intrinsics['cx'],\n                cy=self.intrinsics['cy']\n            )\n        )\n\n        return pcd\n\n    def segment_objects(self, point_cloud, cluster_tolerance=0.02, min_cluster_size=100):\n        # Downsample point cloud\n        downsampled = point_cloud.voxel_down_sample(voxel_size=0.01)\n\n        # Segment plane (ground)\n        plane_model, inliers = downsampled.segment_plane(\n            distance_threshold=0.01,\n            ransac_n=3,\n            num_iterations=1000\n        )\n\n        # Remove ground plane\n        object_cloud = downsampled.select_by_index(inliers, invert=True)\n\n        # Extract clusters\n        labels = np.array(object_cloud.cluster_dbscan(\n            eps=cluster_tolerance,\n            min_points=min_cluster_size\n        ))\n\n        return object_cloud, labels\n"})}),"\n",(0,t.jsx)(n.h2,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,t.jsx)(n.h3,{id:"deep-learning-for-object-detection",children:"Deep Learning for Object Detection"}),"\n",(0,t.jsx)(n.p,{children:"Modern humanoid robots use deep learning models for object detection and recognition:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\nimport torchvision\nfrom torchvision import transforms\nimport cv2\n\nclass ObjectDetector:\n    def __init__(self, model_path=None):\n        # Load pre-trained model (e.g., YOLO or Faster R-CNN)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        if model_path:\n            self.model = torch.load(model_path)\n        else:\n            # Use pre-trained COCO model\n            self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n        self.model.to(self.device)\n        self.model.eval()\n\n        # COCO dataset class names\n        self.coco_names = [\n            '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n            'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n            'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n            'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n            'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n            'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n            'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n            'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n            'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n        # Image transforms\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n    def detect_objects(self, image, confidence_threshold=0.5):\n        # Preprocess image\n        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n\n        # Run inference\n        with torch.no_grad():\n            predictions = self.model(image_tensor)\n\n        # Process predictions\n        boxes = predictions[0]['boxes'].cpu().numpy()\n        labels = predictions[0]['labels'].cpu().numpy()\n        scores = predictions[0]['scores'].cpu().numpy()\n\n        # Filter by confidence\n        valid_indices = scores >= confidence_threshold\n        filtered_boxes = boxes[valid_indices]\n        filtered_labels = labels[valid_indices]\n        filtered_scores = scores[valid_indices]\n\n        # Convert to result format\n        results = []\n        for box, label, score in zip(filtered_boxes, filtered_labels, filtered_scores):\n            results.append({\n                'bbox': box,\n                'label': self.coco_names[label],\n                'confidence': score\n            })\n\n        return results\n"})}),"\n",(0,t.jsx)(n.h2,{id:"visual-slam-for-humanoid-robots",children:"Visual SLAM for Humanoid Robots"}),"\n",(0,t.jsx)(n.p,{children:"Visual SLAM (Simultaneous Localization and Mapping) is crucial for humanoid robots to navigate unknown environments:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\nclass VisualSLAM:\n    def __init__(self):\n        # Feature detector and descriptor\n        self.detector = cv2.SIFT_create()\n        self.matcher = cv2.BFMatcher()\n\n        # Camera parameters\n        self.fx, self.fy = 525.0, 525.0  # Focal lengths\n        self.cx, self.cy = 319.5, 239.5  # Principal points\n\n        # Pose tracking\n        self.current_pose = np.eye(4)\n        self.keyframes = []\n        self.map_points = []\n\n    def process_frame(self, image, timestamp):\n        # Detect features\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\n\n        if len(self.keyframes) == 0:\n            # First frame - initialize\n            self.keyframes.append({\n                'image': image,\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'pose': self.current_pose.copy(),\n                'timestamp': timestamp\n            })\n            return self.current_pose\n\n        # Match with previous frame\n        prev_frame = self.keyframes[-1]\n        matches = self.matcher.knnMatch(\n            prev_frame['descriptors'],\n            descriptors,\n            k=2\n        )\n\n        # Apply Lowe's ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < 0.7 * n.distance:\n                    good_matches.append(m)\n\n        if len(good_matches) >= 10:\n            # Extract matched points\n            prev_pts = np.float32([prev_frame['keypoints'][m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n            curr_pts = np.float32([keypoints[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n\n            # Compute essential matrix and pose\n            E, mask = cv2.findEssentialMat(\n                curr_pts, prev_pts,\n                focal=self.fx, pp=(self.cx, self.cy),\n                method=cv2.RANSAC, threshold=1.0\n            )\n\n            if E is not None:\n                # Recover pose\n                _, R, t, _ = cv2.recoverPose(E, curr_pts, prev_pts,\n                                           focal=self.fx, pp=(self.cx, self.cy))\n\n                # Update current pose\n                T = np.eye(4)\n                T[:3, :3] = R\n                T[:3, 3] = t.flatten()\n                self.current_pose = self.current_pose @ np.linalg.inv(T)\n\n        # Store current frame as keyframe if enough motion detected\n        if self.should_add_keyframe():\n            self.keyframes.append({\n                'image': image,\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'pose': self.current_pose.copy(),\n                'timestamp': timestamp\n            })\n\n        return self.current_pose\n\n    def should_add_keyframe(self):\n        # Simple heuristic: add keyframe if enough frames have passed\n        # or if there are enough new map points\n        return len(self.keyframes) == 0 or len(self.keyframes) % 10 == 0\n"})}),"\n",(0,t.jsx)(n.h2,{id:"face-detection-and-recognition",children:"Face Detection and Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots often need to detect and recognize human faces for social interaction:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import cv2\nimport face_recognition\nimport numpy as np\n\nclass FaceRecognitionSystem:\n    def __init__(self):\n        # Load known faces\n        self.known_face_encodings = []\n        self.known_face_names = []\n\n    def add_known_face(self, image_path, name):\n        # Load image and encode face\n        image = face_recognition.load_image_file(image_path)\n        face_encodings = face_recognition.face_encodings(image)\n\n        if len(face_encodings) > 0:\n            self.known_face_encodings.append(face_encodings[0])\n            self.known_face_names.append(name)\n\n    def recognize_faces(self, image):\n        # Find all face locations and encodings in the image\n        face_locations = face_recognition.face_locations(image)\n        face_encodings = face_recognition.face_encodings(image, face_locations)\n\n        results = []\n        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n            # Compare face with known faces\n            matches = face_recognition.compare_faces(\n                self.known_face_encodings,\n                face_encoding\n            )\n            name = \"Unknown\"\n\n            # Calculate face distances\n            face_distances = face_recognition.face_distance(\n                self.known_face_encodings,\n                face_encoding\n            )\n\n            if len(face_distances) > 0:\n                best_match_index = np.argmin(face_distances)\n                if matches[best_match_index]:\n                    name = self.known_face_names[best_match_index]\n\n            results.append({\n                'name': name,\n                'bbox': (left, top, right, bottom),\n                'confidence': 1 - min(face_distances) if len(face_distances) > 0 else 0\n            })\n\n        return results\n"})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,t.jsx)(n.h3,{id:"image-processing-node",children:"Image Processing Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nfrom std_msgs.msg import String\nimport cv2\n\nclass VisionNode(Node):\n    def __init__(self):\n        super().__init__('vision_node')\n\n        # Create subscribers and publishers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/rgb/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.detection_pub = self.create_publisher(\n            String,\n            '/vision/detections',\n            10\n        )\n\n        # Initialize computer vision components\n        self.cv_bridge = CvBridge()\n        self.object_detector = ObjectDetector()\n\n        self.get_logger().info('Vision node initialized')\n\n    def image_callback(self, msg):\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Run object detection\n            detections = self.object_detector.detect_objects(cv_image)\n\n            # Publish results\n            result_msg = String()\n            result_msg.data = str(detections)\n            self.detection_pub.publish(result_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n"})}),"\n",(0,t.jsx)(n.h2,{id:"challenges-in-humanoid-robot-vision",children:"Challenges in Humanoid Robot Vision"}),"\n",(0,t.jsx)(n.h3,{id:"real-time-processing",children:"Real-time Processing"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots require real-time visual processing while maintaining balance and other tasks:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Efficient algorithms optimized for embedded systems"}),"\n",(0,t.jsx)(n.li,{children:"Multi-threading to separate vision processing from control"}),"\n",(0,t.jsx)(n.li,{children:"Prioritization of critical visual tasks"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"varying-lighting-conditions",children:"Varying Lighting Conditions"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots operate in diverse lighting conditions:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Adaptive exposure control"}),"\n",(0,t.jsx)(n.li,{children:"Image enhancement algorithms"}),"\n",(0,t.jsx)(n.li,{children:"Robust feature detection across lighting variations"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"motion-artifacts",children:"Motion Artifacts"}),"\n",(0,t.jsx)(n.p,{children:"Moving robots introduce motion blur and vibration:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Image stabilization techniques"}),"\n",(0,t.jsx)(n.li,{children:"Motion compensation algorithms"}),"\n",(0,t.jsx)(n.li,{children:"Temporal filtering to reduce noise"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practice-tasks",children:"Practice Tasks"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement a simple object detection system using a pre-trained model"}),"\n",(0,t.jsx)(n.li,{children:"Create a visual SLAM system for a humanoid robot simulation"}),"\n",(0,t.jsx)(n.li,{children:"Develop face recognition capabilities for human-robot interaction"}),"\n",(0,t.jsx)(n.li,{children:"Integrate vision processing with the robot's navigation system"}),"\n",(0,t.jsx)(n.li,{children:"Test vision algorithms under different lighting and motion conditions"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Computer vision enables humanoid robots to perceive and understand their environment. By combining traditional computer vision techniques with deep learning and specialized algorithms for humanoid platforms, robots can perform complex visual tasks essential for autonomous operation."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);