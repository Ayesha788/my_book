"use strict";(globalThis.webpackChunkmy_book_site=globalThis.webpackChunkmy_book_site||[]).push([[0],{1259:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"chapter10-hri","title":"Chapter 10: Human-Robot Interaction for Humanoid Systems","description":"Introduction to Human-Robot Interaction","source":"@site/docs/chapter10-hri.md","sourceDirName":".","slug":"/chapter10-hri","permalink":"/my_book/docs/chapter10-hri","draft":false,"unlisted":false,"editUrl":"https://github.com/Ayesha788/my_book/tree/main/docs/chapter10-hri.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 9: Learning and Adaptation for Humanoid Robots","permalink":"/my_book/docs/chapter9-learning"},"next":{"title":"Chapter 11: Safety and Ethics in Humanoid Robotics","permalink":"/my_book/docs/chapter11-safety"}}');var i=t(4848),s=t(8453);const a={},r="Chapter 10: Human-Robot Interaction for Humanoid Systems",l={},c=[{value:"Introduction to Human-Robot Interaction",id:"introduction-to-human-robot-interaction",level:2},{value:"Social Robotics Principles",id:"social-robotics-principles",level:2},{value:"Anthropomorphic Design Considerations",id:"anthropomorphic-design-considerations",level:3},{value:"Natural Language Processing for HRI",id:"natural-language-processing-for-hri",level:2},{value:"Speech Recognition and Understanding",id:"speech-recognition-and-understanding",level:3},{value:"Gesture Recognition and Generation",id:"gesture-recognition-and-generation",level:2},{value:"Human Gesture Recognition",id:"human-gesture-recognition",level:3},{value:"Robot Gesture Generation",id:"robot-gesture-generation",level:3},{value:"Emotional Interaction and Expression",id:"emotional-interaction-and-expression",level:2},{value:"Emotional State Management",id:"emotional-state-management",level:3},{value:"Multi-Modal Interaction Framework",id:"multi-modal-interaction-framework",level:2},{value:"Integration of Multiple Interaction Modalities",id:"integration-of-multiple-interaction-modalities",level:3},{value:"ROS 2 Integration for HRI",id:"ros-2-integration-for-hri",level:2},{value:"Human-Robot Interaction Node",id:"human-robot-interaction-node",level:3},{value:"Challenges in Human-Robot Interaction",id:"challenges-in-human-robot-interaction",level:2},{value:"Social Acceptance",id:"social-acceptance",level:3},{value:"Safety and Trust",id:"safety-and-trust",level:3},{value:"Technical Challenges",id:"technical-challenges",level:3},{value:"Practice Tasks",id:"practice-tasks",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-10-human-robot-interaction-for-humanoid-systems",children:"Chapter 10: Human-Robot Interaction for Humanoid Systems"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-human-robot-interaction",children:"Introduction to Human-Robot Interaction"}),"\n",(0,i.jsx)(n.p,{children:"Human-Robot Interaction (HRI) is a critical aspect of humanoid robotics, focusing on how humans and robots can effectively communicate, collaborate, and work together. Unlike traditional industrial robots, humanoid robots are designed to operate in human-centered environments and interact naturally with people."}),"\n",(0,i.jsx)(n.h2,{id:"social-robotics-principles",children:"Social Robotics Principles"}),"\n",(0,i.jsx)(n.h3,{id:"anthropomorphic-design-considerations",children:"Anthropomorphic Design Considerations"}),"\n",(0,i.jsx)(n.p,{children:"Humanoid robots leverage human-like features to facilitate natural interaction:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\nclass GazeBehavior(Enum):\n    ATTENTIVE = "attentive"\n    CORDIAL = "cordial"\n    FOCUSED = "focused"\n    AVOIDANT = "avoidant"\n\nclass GestureType(Enum):\n    GREETING = "greeting"\n    POINTING = "pointing"\n    EMPHASIS = "emphasis"\n    REGULATORY = "regulatory"\n    ADAPTIVE = "adaptive"\n\n@dataclass\nclass SocialState:\n    engagement_level: float  # 0.0 to 1.0\n    attention_direction: np.ndarray  # 3D vector\n    emotional_state: str  # happy, neutral, concerned, etc.\n    proximity_comfort: float  # Personal space comfort level\n\nclass SocialBehaviorController:\n    def __init__(self, robot_id: str):\n        self.robot_id = robot_id\n        self.social_state = SocialState(\n            engagement_level=0.5,\n            attention_direction=np.array([0.0, 0.0, 1.0]),\n            emotional_state="neutral",\n            proximity_comfort=0.8\n        )\n        self.human_tracking = {}  # Track multiple humans\n        self.social_rules = self.define_social_rules()\n\n    def define_social_rules(self) -> Dict:\n        """Define social behavior rules based on context"""\n        return {\n            "personal_space": 0.8,  # meters\n            "social_space": 1.2,   # meters\n            "public_space": 3.6,   # meters\n            "gaze_duration_min": 0.5,  # seconds\n            "gaze_duration_max": 3.0,  # seconds\n        }\n\n    def update_human_tracking(self, human_id: str, position: np.ndarray, is_looking_at_robot: bool):\n        """Update tracking information for a human"""\n        self.human_tracking[human_id] = {\n            \'position\': position,\n            \'is_looking_at_robot\': is_looking_at_robot,\n            \'last_seen\': self.get_current_time(),\n            \'engagement_score\': self.calculate_engagement_score(position, is_looking_at_robot)\n        }\n\n    def calculate_engagement_score(self, position: np.ndarray, is_looking: bool) -> float:\n        """Calculate how engaged a human is with the robot"""\n        # Distance-based engagement (closer humans are more likely to engage)\n        distance = np.linalg.norm(position)\n        distance_factor = max(0, 1 - distance / 2.0)  # Higher engagement when closer\n\n        # Looking direction factor\n        looking_factor = 1.0 if is_looking else 0.3\n\n        return distance_factor * looking_factor\n\n    def select_appropriate_behavior(self, context: Dict) -> Dict:\n        """Select appropriate social behavior based on context"""\n        behaviors = {}\n\n        # Gaze behavior selection\n        if context.get(\'human_count\', 0) == 1:\n            behaviors[\'gaze\'] = self.select_gaze_behavior(context)\n        else:\n            behaviors[\'gaze\'] = self.select_group_gaze_behavior(context)\n\n        # Gesture selection\n        behaviors[\'gesture\'] = self.select_gesture(context)\n\n        # Proximity management\n        behaviors[\'proximity\'] = self.manage_proximity(context)\n\n        return behaviors\n\n    def select_gaze_behavior(self, context: Dict) -> GazeBehavior:\n        """Select appropriate gaze behavior"""\n        human_id = context.get(\'target_human\', \'\')\n        if human_id in self.human_tracking:\n            engagement = self.human_tracking[human_id][\'engagement_score\']\n            if engagement > 0.7:\n                return GazeBehavior.ATTENTIVE\n            elif engagement > 0.4:\n                return GazeBehavior.CORDIAL\n            else:\n                return GazeBehavior.AVOIDANT\n        return GazeBehavior.AVOIDANT\n\n    def select_gesture(self, context: Dict) -> GestureType:\n        """Select appropriate gesture based on interaction context"""\n        interaction_type = context.get(\'interaction_type\', \'unknown\')\n        if interaction_type == \'greeting\':\n            return GestureType.GREETING\n        elif interaction_type == \'instruction\':\n            return GestureType.POINTING\n        elif interaction_type == \'emphasis\':\n            return GestureType.EMPHASIS\n        else:\n            return GestureType.CORDIAL\n'})}),"\n",(0,i.jsx)(n.h2,{id:"natural-language-processing-for-hri",children:"Natural Language Processing for HRI"}),"\n",(0,i.jsx)(n.h3,{id:"speech-recognition-and-understanding",children:"Speech Recognition and Understanding"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import speech_recognition as sr\nimport nltk\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\nimport numpy as np\n\nclass NaturalLanguageProcessor:\n    def __init__(self):\n        # Initialize speech recognition\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Initialize NLP models\n        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        self.intent_model = AutoModelForSequenceClassification.from_pretrained(\n            \"microsoft/DialoGPT-medium\"\n        )\n\n        # Intent classification pipeline\n        self.intent_classifier = pipeline(\n            \"text-classification\",\n            model=\"microsoft/DialoGPT-medium\"\n        )\n\n        # Predefined commands and intents\n        self.intent_patterns = {\n            'greeting': ['hello', 'hi', 'hey', 'good morning', 'good evening'],\n            'navigation': ['go to', 'move to', 'navigate to', 'walk to', 'go'],\n            'manipulation': ['pick up', 'grasp', 'take', 'get', 'bring me'],\n            'information': ['what', 'how', 'when', 'where', 'who', 'tell me'],\n            'social': ['how are you', 'what are you doing', 'nice to meet you'],\n            'farewell': ['goodbye', 'bye', 'see you', 'thank you', 'thanks']\n        }\n\n    def recognize_speech(self, audio_file: str = None) -> str:\n        \"\"\"Recognize speech from audio input\"\"\"\n        try:\n            if audio_file:\n                with sr.AudioFile(audio_file) as source:\n                    audio = self.recognizer.record(source)\n            else:\n                with self.microphone as source:\n                    self.recognizer.adjust_for_ambient_noise(source)\n                    print(\"Listening...\")\n                    audio = self.recognizer.listen(source)\n\n            # Use Google Speech Recognition (or other engines)\n            text = self.recognizer.recognize_google(audio)\n            return text.lower()\n\n        except sr.UnknownValueError:\n            return \"\"\n        except sr.RequestError:\n            return \"\"\n\n    def classify_intent(self, text: str) -> Dict:\n        \"\"\"Classify the intent of the given text\"\"\"\n        # Simple keyword-based classification first\n        for intent, patterns in self.intent_patterns.items():\n            if any(pattern in text for pattern in patterns):\n                return {\n                    'intent': intent,\n                    'confidence': 0.8,  # Placeholder confidence\n                    'entities': self.extract_entities(text)\n                }\n\n        # If no keyword match, use more sophisticated NLP\n        result = self.intent_classifier(text)\n        return {\n            'intent': result['label'],\n            'confidence': result['score'],\n            'entities': self.extract_entities(text)\n        }\n\n    def extract_entities(self, text: str) -> List[Dict]:\n        \"\"\"Extract named entities from text\"\"\"\n        entities = []\n\n        # Simple entity extraction (in practice, use spaCy or similar)\n        words = text.split()\n        for i, word in enumerate(words):\n            if word in ['kitchen', 'living room', 'bedroom', 'table', 'chair', 'cup', 'book']:\n                entities.append({\n                    'text': word,\n                    'type': 'LOCATION' if word in ['kitchen', 'living room', 'bedroom'] else 'OBJECT',\n                    'position': i\n                })\n\n        return entities\n\n    def generate_response(self, user_input: str, context: Dict = None) -> str:\n        \"\"\"Generate appropriate response to user input\"\"\"\n        intent_info = self.classify_intent(user_input)\n\n        if intent_info['intent'] == 'greeting':\n            return self.generate_greeting_response()\n        elif intent_info['intent'] == 'navigation':\n            return self.generate_navigation_response(intent_info['entities'])\n        elif intent_info['intent'] == 'manipulation':\n            return self.generate_manipulation_response(intent_info['entities'])\n        elif intent_info['intent'] == 'information':\n            return self.generate_information_response(user_input)\n        elif intent_info['intent'] == 'social':\n            return self.generate_social_response()\n        elif intent_info['intent'] == 'farewell':\n            return self.generate_farewell_response()\n        else:\n            return self.generate_default_response()\n\n    def generate_greeting_response(self) -> str:\n        \"\"\"Generate greeting response\"\"\"\n        responses = [\n            \"Hello! It's nice to meet you.\",\n            \"Hi there! How can I help you today?\",\n            \"Good to see you! What would you like to do?\"\n        ]\n        import random\n        return random.choice(responses)\n\n    def generate_navigation_response(self, entities: List[Dict]) -> str:\n        \"\"\"Generate navigation response\"\"\"\n        if entities:\n            location = entities[0]['text']\n            return f\"I can help you navigate to the {location}. Please follow me.\"\n        return \"I can help you navigate. Where would you like to go?\"\n\n    def generate_manipulation_response(self, entities: List[Dict]) -> str:\n        \"\"\"Generate manipulation response\"\"\"\n        if entities:\n            obj = entities[0]['text']\n            return f\"I can help you with the {obj}. I'll retrieve it for you.\"\n        return \"I can help you with that. What would you like me to do?\"\n\n    def generate_default_response(self) -> str:\n        \"\"\"Generate default response when intent is unclear\"\"\"\n        return \"I'm not sure I understand. Could you please rephrase that?\"\n"})}),"\n",(0,i.jsx)(n.h2,{id:"gesture-recognition-and-generation",children:"Gesture Recognition and Generation"}),"\n",(0,i.jsx)(n.h3,{id:"human-gesture-recognition",children:"Human Gesture Recognition"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import cv2\nimport mediapipe as mp\nimport numpy as np\nfrom enum import Enum\n\nclass Gesture(Enum):\n    WAVING = "waving"\n    POINTING = "pointing"\n    THUMBS_UP = "thumbs_up"\n    THUMBS_DOWN = "thumbs_down"\n    STOP = "stop"\n    COME_HERE = "come_here"\n\nclass GestureRecognizer:\n    def __init__(self):\n        # Initialize MediaPipe for pose and hand tracking\n        self.mp_pose = mp.solutions.pose\n        self.mp_hands = mp.solutions.hands\n        self.mp_drawing = mp.solutions.drawing_utils\n\n        self.pose = self.mp_pose.Pose(\n            static_image_mode=False,\n            model_complexity=1,\n            enable_segmentation=False,\n            min_detection_confidence=0.5\n        )\n\n        self.hands = self.mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=2,\n            min_detection_confidence=0.5\n        )\n\n    def recognize_gestures(self, image):\n        """Recognize gestures from image"""\n        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Process pose\n        pose_results = self.pose.process(rgb_image)\n        # Process hands\n        hand_results = self.hands.process(rgb_image)\n\n        recognized_gestures = []\n\n        if hand_results.multi_hand_landmarks:\n            for hand_landmarks in hand_results.multi_hand_landmarks:\n                gesture = self.analyze_hand_gesture(hand_landmarks, image.shape)\n                if gesture:\n                    recognized_gestures.append(gesture)\n\n        if pose_results.pose_landmarks:\n            pose_gesture = self.analyze_body_gesture(pose_results.pose_landmarks)\n            if pose_gesture:\n                recognized_gestures.append(pose_gesture)\n\n        return recognized_gestures\n\n    def analyze_hand_gesture(self, hand_landmarks, image_shape):\n        """Analyze hand landmarks to recognize specific gestures"""\n        # Get landmark coordinates\n        landmarks = []\n        for landmark in hand_landmarks.landmark:\n            x = int(landmark.x * image_shape[1])\n            y = int(landmark.y * image_shape[0])\n            landmarks.append((x, y))\n\n        # Calculate distances between key points\n        thumb_tip = landmarks[4]\n        index_tip = landmarks[8]\n        middle_tip = landmarks[12]\n        ring_tip = landmarks[16]\n        pinky_tip = landmarks[20]\n\n        # Waving: moving hand side to side\n        # This would require tracking movement over time\n\n        # Thumbs up: thumb up, other fingers down\n        if (thumb_tip[1] < index_tip[1] and  # Thumb higher than index\n            thumb_tip[1] < middle_tip[1] and  # Thumb higher than middle\n            thumb_tip[1] < ring_tip[1] and    # Thumb higher than ring\n            thumb_tip[1] < pinky_tip[1]):     # Thumb higher than pinky\n            return Gesture.THUMBS_UP\n\n        # Stop: palm facing forward, fingers extended\n        # Come here: index finger pointing toward robot\n\n        # Pointing: index finger extended, other fingers curled\n        if (index_tip[1] < middle_tip[1] and  # Index higher than middle\n            index_tip[1] < ring_tip[1] and    # Index higher than ring\n            index_tip[1] < pinky_tip[1]):     # Index higher than pinky\n            return Gesture.POINTING\n\n        return None\n\n    def analyze_body_gesture(self, pose_landmarks):\n        """Analyze body pose to recognize gestures"""\n        # Extract key pose landmarks\n        landmarks = pose_landmarks.landmark\n\n        # Access specific landmarks by index\n        left_shoulder = landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER]\n        right_shoulder = landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]\n        left_arm = landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST]\n        right_arm = landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST]\n\n        # Analyze based on relative positions\n        # For example, waving could be detected by arm movement over time\n\n        return None  # Placeholder\n'})}),"\n",(0,i.jsx)(n.h3,{id:"robot-gesture-generation",children:"Robot Gesture Generation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport math\n\nclass GestureGenerator:\n    def __init__(self, robot_joints):\n        self.robot_joints = robot_joints  # Joint configuration for the humanoid\n        self.gesture_sequences = self.define_gestures()\n\n    def define_gestures(self):\n        """Define gesture sequences for different gestures"""\n        return {\n            \'waving\': self.create_waving_gesture(),\n            \'pointing\': self.create_pointing_gesture(),\n            \'greeting\': self.create_greeting_gesture(),\n            \'acknowledging\': self.create_acknowledging_gesture()\n        }\n\n    def create_waving_gesture(self):\n        """Create a waving gesture sequence"""\n        sequence = []\n        base_pos = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # Base joint positions\n\n        # Wave motion: move arm up and down\n        for t in np.linspace(0, 2*np.pi, 20):  # 20 time steps\n            # Elbow and wrist joints move in wave pattern\n            elbow_pos = 0.5 * math.sin(t)\n            wrist_pos = 0.3 * math.sin(2*t)\n\n            joint_pos = base_pos.copy()\n            joint_pos[1] = elbow_pos  # Elbow joint\n            joint_pos[2] = wrist_pos  # Wrist joint\n\n            sequence.append({\n                \'joints\': joint_pos,\n                \'duration\': 0.1  # 100ms per step\n            })\n\n        return sequence\n\n    def create_pointing_gesture(self):\n        """Create a pointing gesture"""\n        sequence = []\n        base_pos = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\n        # Move arm to pointing position\n        for i in range(10):\n            progress = i / 9.0  # 0 to 1\n            joint_pos = base_pos.copy()\n            joint_pos[0] = 0.8 * progress  # Shoulder\n            joint_pos[1] = 0.6 * progress  # Elbow\n            joint_pos[2] = 0.4 * progress  # Wrist\n\n            sequence.append({\n                \'joints\': joint_pos,\n                \'duration\': 0.05\n            })\n\n        # Hold position briefly\n        for i in range(5):\n            sequence.append({\n                \'joints\': sequence[-1][\'joints\'],\n                \'duration\': 0.1\n            })\n\n        # Return to neutral\n        for i in range(10):\n            progress = 1 - (i / 9.0)  # 1 to 0\n            joint_pos = base_pos.copy()\n            joint_pos[0] = 0.8 * progress\n            joint_pos[1] = 0.6 * progress\n            joint_pos[2] = 0.4 * progress\n\n            sequence.append({\n                \'joints\': joint_pos,\n                \'duration\': 0.05\n            })\n\n        return sequence\n\n    def execute_gesture(self, gesture_name: str):\n        """Execute a predefined gesture"""\n        if gesture_name in self.gesture_sequences:\n            sequence = self.gesture_sequences[gesture_name]\n            return self.execute_sequence(sequence)\n        else:\n            print(f"Gesture \'{gesture_name}\' not defined")\n            return False\n\n    def execute_sequence(self, sequence):\n        """Execute a sequence of joint positions"""\n        for step in sequence:\n            # In a real robot, this would send commands to joint controllers\n            target_joints = step[\'joints\']\n            duration = step[\'duration\']\n\n            # Simulate execution\n            print(f"Moving to joints: {target_joints} for {duration}s")\n\n            # In ROS 2, you would publish joint trajectory messages here\n            # self.joint_trajectory_publisher.publish(trajectory_msg)\n\n        return True\n'})}),"\n",(0,i.jsx)(n.h2,{id:"emotional-interaction-and-expression",children:"Emotional Interaction and Expression"}),"\n",(0,i.jsx)(n.h3,{id:"emotional-state-management",children:"Emotional State Management"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom enum import Enum\nfrom typing import Dict, List\n\nclass EmotionalState(Enum):\n    HAPPY = \"happy\"\n    SAD = \"sad\"\n    ANGRY = \"angry\"\n    SURPRISED = \"surprised\"\n    NEUTRAL = \"neutral\"\n    CONFUSED = \"confused\"\n    CONCERNED = \"concerned\"\n\nclass EmotionalStateEstimator:\n    def __init__(self):\n        self.current_emotion = EmotionalState.NEUTRAL\n        self.emotion_history = []\n        self.confidence_threshold = 0.6\n\n    def estimate_emotion_from_audio(self, audio_features: Dict) -> EmotionalState:\n        \"\"\"Estimate emotion from audio features (tone, pitch, speed)\"\"\"\n        # Simplified emotion estimation\n        pitch = audio_features.get('pitch', 0.5)\n        speed = audio_features.get('speed', 1.0)\n        volume = audio_features.get('volume', 0.5)\n\n        if speed > 1.5 and pitch > 0.7:  # Fast and high pitch\n            return EmotionalState.SURPRISED\n        elif speed < 0.7 and pitch < 0.3:  # Slow and low pitch\n            return EmotionalState.SAD\n        elif volume > 0.8 and pitch > 0.6:  # Loud and high pitch\n            return EmotionalState.ANGRY\n        else:\n            return EmotionalState.NEUTRAL\n\n    def estimate_emotion_from_vision(self, facial_features: Dict) -> EmotionalState:\n        \"\"\"Estimate emotion from facial expression\"\"\"\n        # Based on facial landmark analysis\n        eyebrow_raised = facial_features.get('eyebrow_raised', False)\n        mouth_open = facial_features.get('mouth_open', False)\n        eyes_wide = facial_features.get('eyes_wide', False)\n\n        if eyebrow_raised and eyes_wide:\n            return EmotionalState.SURPRISED\n        elif mouth_open and eyes_wide:\n            return EmotionalState.SURPRISED\n        elif not mouth_open and not eyes_wide and not eyebrow_raised:\n            return EmotionalState.NEUTRAL\n        else:\n            return EmotionalState.CONFUSED\n\n    def estimate_emotion_from_context(self, context: Dict) -> EmotionalState:\n        \"\"\"Estimate emotion based on interaction context\"\"\"\n        recent_events = context.get('recent_events', [])\n        user_tone = context.get('user_tone', 'neutral')\n\n        if 'error' in recent_events:\n            return EmotionalState.CONCERNED\n        elif user_tone == 'frustrated':\n            return EmotionalState.CONCERNED\n        elif user_tone == 'happy':\n            return EmotionalState.HAPPY\n        else:\n            return EmotionalState.NEUTRAL\n\nclass EmotionalExpressionController:\n    def __init__(self):\n        self.face_expression_map = {\n            EmotionalState.HAPPY: {'mouth': 'smile', 'eyebrows': 'neutral', 'eyes': 'normal'},\n            EmotionalState.SAD: {'mouth': 'frown', 'eyebrows': 'dropped', 'eyes': 'droopy'},\n            EmotionalState.ANGRY: {'mouth': 'tight', 'eyebrows': 'furrowed', 'eyes': 'narrow'},\n            EmotionalState.SURPRISED: {'mouth': 'open', 'eyebrows': 'raised', 'eyes': 'wide'},\n            EmotionalState.NEUTRAL: {'mouth': 'neutral', 'eyebrows': 'neutral', 'eyes': 'normal'},\n            EmotionalState.CONFUSED: {'mouth': 'slightly_open', 'eyebrows': 'one_raised', 'eyes': 'looking_around'},\n            EmotionalState.CONCERNED: {'mouth': 'tight', 'eyebrows': 'furrowed', 'eyes': 'wide'},\n        }\n\n    def generate_emotional_response(self, perceived_emotion: EmotionalState, context: Dict) -> Dict:\n        \"\"\"Generate appropriate emotional response\"\"\"\n        response = {\n            'face_expression': self.face_expression_map[perceived_emotion],\n            'vocal_tone': self.get_vocal_tone_for_emotion(perceived_emotion),\n            'body_posture': self.get_posture_for_emotion(perceived_emotion),\n            'response_text': self.get_response_text_for_emotion(perceived_emotion, context)\n        }\n        return response\n\n    def get_vocal_tone_for_emotion(self, emotion: EmotionalState) -> Dict:\n        \"\"\"Get vocal tone parameters for an emotion\"\"\"\n        tone_map = {\n            EmotionalState.HAPPY: {'pitch': 1.2, 'speed': 1.1, 'volume': 1.0},\n            EmotionalState.SAD: {'pitch': 0.8, 'speed': 0.7, 'volume': 0.8},\n            EmotionalState.ANGRY: {'pitch': 1.1, 'speed': 1.5, 'volume': 1.3},\n            EmotionalState.SURPRISED: {'pitch': 1.4, 'speed': 1.2, 'volume': 1.1},\n            EmotionalState.NEUTRAL: {'pitch': 1.0, 'speed': 1.0, 'volume': 1.0},\n            EmotionalState.CONFUSED: {'pitch': 0.9, 'speed': 0.9, 'volume': 0.9},\n            EmotionalState.CONCERNED: {'pitch': 0.95, 'speed': 0.8, 'volume': 0.9}\n        }\n        return tone_map.get(emotion, tone_map[EmotionalState.NEUTRAL])\n\n    def get_posture_for_emotion(self, emotion: EmotionalState) -> str:\n        \"\"\"Get body posture for an emotion\"\"\"\n        posture_map = {\n            EmotionalState.HAPPY: 'upright_and_open',\n            EmotionalState.SAD: 'slightly_hunched',\n            EmotionalState.ANGRY: 'stiff_and_direct',\n            EmotionalState.SURPRISED: 'leaning_forward',\n            EmotionalState.NEUTRAL: 'normal_standing',\n            EmotionalState.CONFUSED: 'slight_head_tilt',\n            EmotionalState.CONCERNED: 'leaning_slightly_forward'\n        }\n        return posture_map.get(emotion, 'normal_standing')\n\n    def get_response_text_for_emotion(self, emotion: EmotionalState, context: Dict) -> str:\n        \"\"\"Get appropriate response text for an emotion\"\"\"\n        response_templates = {\n            EmotionalState.HAPPY: \"I'm glad you're happy! How can I help?\",\n            EmotionalState.SAD: \"I'm sorry you're feeling down. Is there anything I can do?\",\n            EmotionalState.ANGRY: \"I understand you're upset. Let me help resolve this.\",\n            EmotionalState.SURPRISED: \"Oh! Did I surprise you? I didn't mean to.\",\n            EmotionalState.NEUTRAL: \"Hello! How can I assist you today?\",\n            EmotionalState.CONFUSED: \"I see you look confused. Let me clarify.\",\n            EmotionalState.CONCERNED: \"I notice you seem concerned. How can I help?\"\n        }\n        return response_templates.get(emotion, \"Hello! How can I help?\")\n"})}),"\n",(0,i.jsx)(n.h2,{id:"multi-modal-interaction-framework",children:"Multi-Modal Interaction Framework"}),"\n",(0,i.jsx)(n.h3,{id:"integration-of-multiple-interaction-modalities",children:"Integration of Multiple Interaction Modalities"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import threading\nimport time\nfrom queue import Queue\n\nclass MultiModalInteractionFramework:\n    def __init__(self):\n        # Initialize all interaction modules\n        self.nlp_processor = NaturalLanguageProcessor()\n        self.gesture_recognizer = GestureRecognizer()\n        self.emotion_estimator = EmotionalStateEstimator()\n        self.social_controller = SocialBehaviorController(\"robot1\")\n        self.gesture_generator = GestureGenerator(robot_joints=[])\n\n        # Queues for different modalities\n        self.speech_queue = Queue()\n        self.vision_queue = Queue()\n        self.tactile_queue = Queue()\n\n        # Interaction state\n        self.current_interaction = None\n        self.interaction_history = []\n\n        # Start processing threads\n        self.speech_thread = threading.Thread(target=self.process_speech_input)\n        self.vision_thread = threading.Thread(target=self.process_vision_input)\n        self.main_thread = threading.Thread(target=self.main_interaction_loop)\n\n    def process_speech_input(self):\n        \"\"\"Continuously process speech input\"\"\"\n        while True:\n            try:\n                # In a real system, this would continuously listen\n                speech_text = self.nlp_processor.recognize_speech()\n                if speech_text:\n                    intent_info = self.nlp_processor.classify_intent(speech_text)\n                    self.speech_queue.put({\n                        'type': 'speech',\n                        'text': speech_text,\n                        'intent': intent_info\n                    })\n                time.sleep(0.1)  # Small delay to prevent busy waiting\n            except Exception as e:\n                print(f\"Speech processing error: {e}\")\n\n    def process_vision_input(self):\n        \"\"\"Continuously process vision input\"\"\"\n        cap = cv2.VideoCapture(0)  # Camera input\n        while True:\n            ret, frame = cap.read()\n            if ret:\n                gestures = self.gesture_recognizer.recognize_gestures(frame)\n                if gestures:\n                    self.vision_queue.put({\n                        'type': 'gesture',\n                        'gestures': gestures,\n                        'timestamp': time.time()\n                    })\n            time.sleep(0.1)\n\n    def main_interaction_loop(self):\n        \"\"\"Main loop for coordinating multi-modal interaction\"\"\"\n        while True:\n            # Process all available inputs\n            self.process_queued_inputs()\n\n            # Generate appropriate response\n            response = self.generate_response()\n\n            # Execute response\n            self.execute_response(response)\n\n            time.sleep(0.05)  # Main loop rate\n\n    def process_queued_inputs(self):\n        \"\"\"Process all queued interaction inputs\"\"\"\n        # Process speech inputs\n        while not self.speech_queue.empty():\n            speech_data = self.speech_queue.get()\n            self.handle_speech_input(speech_data)\n\n        # Process vision inputs\n        while not self.vision_queue.empty():\n            vision_data = self.vision_queue.get()\n            self.handle_vision_input(vision_data)\n\n    def handle_speech_input(self, speech_data):\n        \"\"\"Handle speech input and update interaction state\"\"\"\n        text = speech_data['text']\n        intent = speech_data['intent']\n\n        # Update social state based on interaction\n        if intent['intent'] == 'greeting':\n            self.social_controller.social_state.engagement_level = 0.8\n            self.social_controller.social_state.emotional_state = \"happy\"\n        elif intent['intent'] == 'farewell':\n            self.social_controller.social_state.engagement_level = 0.2\n\n        # Store in interaction history\n        self.interaction_history.append({\n            'type': 'speech',\n            'content': text,\n            'intent': intent,\n            'timestamp': time.time()\n        })\n\n    def handle_vision_input(self, vision_data):\n        \"\"\"Handle vision input and update interaction state\"\"\"\n        gestures = vision_data['gestures']\n\n        for gesture in gestures:\n            if gesture == Gesture.WAVING:\n                # Increase engagement level\n                self.social_controller.social_state.engagement_level = min(\n                    1.0, self.social_controller.social_state.engagement_level + 0.3\n                )\n                # Generate waving response\n                self.gesture_generator.execute_gesture('waving')\n\n        # Store in interaction history\n        self.interaction_history.append({\n            'type': 'gesture',\n            'content': [g.value for g in gestures],\n            'timestamp': time.time()\n        })\n\n    def generate_response(self):\n        \"\"\"Generate multimodal response based on current state\"\"\"\n        # Analyze current context\n        context = {\n            'social_state': self.social_controller.social_state,\n            'recent_interactions': self.interaction_history[-5:],  # Last 5 interactions\n            'engagement_level': self.social_controller.social_state.engagement_level\n        }\n\n        # Select appropriate behaviors\n        behaviors = self.social_controller.select_appropriate_behavior(context)\n\n        # Generate response components\n        response = {\n            'speech': self.generate_speech_response(context),\n            'gesture': behaviors['gesture'],\n            'gaze': behaviors['gaze'],\n            'emotional_expression': self.generate_emotional_response(context)\n        }\n\n        return response\n\n    def generate_speech_response(self, context):\n        \"\"\"Generate appropriate speech response\"\"\"\n        engagement = context['engagement_level']\n\n        if engagement > 0.7:\n            return \"I'm happy to help you with that!\"\n        elif engagement > 0.4:\n            return \"I can assist you. What would you like to do?\"\n        else:\n            return \"Hello there! How can I help you today?\"\n\n    def generate_emotional_response(self, context):\n        \"\"\"Generate emotional expression response\"\"\"\n        engagement = context['engagement_level']\n\n        if engagement > 0.7:\n            return EmotionalState.HAPPY\n        elif engagement > 0.4:\n            return EmotionalState.NEUTRAL\n        else:\n            return EmotionalState.CONCERNED\n\n    def execute_response(self, response):\n        \"\"\"Execute the multimodal response\"\"\"\n        # Execute speech\n        print(f\"Robot says: {response['speech']}\")\n\n        # Execute gesture\n        if response['gesture']:\n            gesture_name = response['gesture'].value\n            self.gesture_generator.execute_gesture(gesture_name)\n\n        # Execute emotional expression\n        if response['emotional_expression']:\n            expr_controller = EmotionalExpressionController()\n            emotion_response = expr_controller.generate_emotional_response(\n                response['emotional_expression'], {}\n            )\n            print(f\"Robot expresses: {emotion_response}\")\n"})}),"\n",(0,i.jsx)(n.h2,{id:"ros-2-integration-for-hri",children:"ROS 2 Integration for HRI"}),"\n",(0,i.jsx)(n.h3,{id:"human-robot-interaction-node",children:"Human-Robot Interaction Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool, Float32\nfrom sensor_msgs.msg import Image, JointState\nfrom geometry_msgs.msg import PointStamped\nfrom hri_msgs.msg import HumanState, RobotExpression, InteractionEvent\n\nclass HRIManager(Node):\n    def __init__(self):\n        super().__init__('hri_manager')\n\n        # Publishers\n        self.speech_pub = self.create_publisher(String, 'robot_speech', 10)\n        self.expression_pub = self.create_publisher(RobotExpression, 'robot_expression', 10)\n        self.gesture_pub = self.create_publisher(String, 'robot_gesture', 10)\n        self.interaction_pub = self.create_publisher(InteractionEvent, 'interaction_events', 10)\n\n        # Subscribers\n        self.speech_sub = self.create_subscription(\n            String, 'human_speech', self.speech_callback, 10\n        )\n        self.image_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10\n        )\n        self.human_state_sub = self.create_subscription(\n            HumanState, 'human_state', self.human_state_callback, 10\n        )\n\n        # Initialize interaction components\n        self.nlp_processor = NaturalLanguageProcessor()\n        self.social_controller = SocialBehaviorController(self.get_namespace())\n        self.gesture_generator = GestureGenerator([])\n\n        # Timer for periodic interaction updates\n        self.interaction_timer = self.create_timer(0.1, self.interaction_step)\n\n        # Internal state\n        self.human_states = {}\n        self.current_interaction = None\n\n        self.get_logger().info('HRI Manager initialized')\n\n    def speech_callback(self, msg: String):\n        \"\"\"Handle incoming human speech\"\"\"\n        intent_info = self.nlp_processor.classify_intent(msg.data.lower())\n\n        # Publish interaction event\n        event_msg = InteractionEvent()\n        event_msg.type = \"speech_input\"\n        event_msg.content = msg.data\n        event_msg.intent = intent_info['intent']\n        self.interaction_pub.publish(event_msg)\n\n        # Generate and publish response\n        response = self.nlp_processor.generate_response(msg.data)\n        response_msg = String()\n        response_msg.data = response\n        self.speech_pub.publish(response_msg)\n\n    def image_callback(self, msg: Image):\n        \"\"\"Process camera image for gesture recognition\"\"\"\n        # Convert ROS Image to OpenCV format and process\n        # This would involve calling gesture recognition algorithms\n        pass\n\n    def human_state_callback(self, msg: HumanState):\n        \"\"\"Update with human state information\"\"\"\n        self.human_states[msg.human_id] = {\n            'position': msg.position,\n            'gaze_direction': msg.gaze_direction,\n            'engagement': msg.engagement_level\n        }\n\n        # Update social controller\n        self.social_controller.update_human_tracking(\n            msg.human_id,\n            np.array([msg.position.x, msg.position.y, msg.position.z]),\n            msg.is_looking_at_robot\n        )\n\n    def interaction_step(self):\n        \"\"\"Main interaction processing step\"\"\"\n        if self.human_states:\n            # Determine primary human for interaction\n            primary_human = self.select_primary_human()\n\n            if primary_human:\n                context = {\n                    'target_human': primary_human,\n                    'human_count': len(self.human_states),\n                    'interaction_type': 'unknown'  # Would be determined from context\n                }\n\n                # Select appropriate behaviors\n                behaviors = self.social_controller.select_appropriate_behavior(context)\n\n                # Execute gaze behavior\n                gaze_msg = String()\n                gaze_msg.data = behaviors['gaze'].value\n                self.gesture_pub.publish(gaze_msg)\n\n                # Execute gesture if appropriate\n                if behaviors['gesture']:\n                    gesture_msg = String()\n                    gesture_msg.data = behaviors['gesture'].value\n                    self.gesture_pub.publish(gesture_msg)\n\n    def select_primary_human(self):\n        \"\"\"Select the primary human for interaction\"\"\"\n        if not self.human_states:\n            return None\n\n        # Select human with highest engagement level\n        primary_human = max(\n            self.human_states.items(),\n            key=lambda x: x[1].get('engagement', 0)\n        )[0]\n\n        return primary_human\n"})}),"\n",(0,i.jsx)(n.h2,{id:"challenges-in-human-robot-interaction",children:"Challenges in Human-Robot Interaction"}),"\n",(0,i.jsx)(n.h3,{id:"social-acceptance",children:"Social Acceptance"}),"\n",(0,i.jsx)(n.p,{children:"Humanoid robots must be designed to be socially acceptable:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Appropriate appearance and behavior"}),"\n",(0,i.jsx)(n.li,{children:"Respect for cultural norms"}),"\n",(0,i.jsx)(n.li,{children:"Privacy considerations"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"safety-and-trust",children:"Safety and Trust"}),"\n",(0,i.jsx)(n.p,{children:"Ensuring safe and trustworthy interactions:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Predictable behavior"}),"\n",(0,i.jsx)(n.li,{children:"Clear communication of robot capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Safe physical interaction"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"technical-challenges",children:"Technical Challenges"}),"\n",(0,i.jsx)(n.p,{children:"Various technical challenges in HRI:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Robust perception in real environments"}),"\n",(0,i.jsx)(n.li,{children:"Real-time processing requirements"}),"\n",(0,i.jsx)(n.li,{children:"Multi-modal integration complexity"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"practice-tasks",children:"Practice Tasks"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement a simple speech recognition and response system"}),"\n",(0,i.jsx)(n.li,{children:"Create gesture recognition using computer vision"}),"\n",(0,i.jsx)(n.li,{children:"Develop emotional expression capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Design a multimodal interaction system"}),"\n",(0,i.jsx)(n.li,{children:"Test HRI system with human subjects in simulation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Human-Robot Interaction is crucial for humanoid robots to effectively collaborate with humans. By implementing natural communication modalities, social behaviors, and emotional expressions, humanoid robots can create more intuitive and engaging interactions that facilitate human-robot collaboration."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var o=t(6540);const i={},s=o.createContext(i);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);