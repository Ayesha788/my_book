# Glossary of Terms

## A

**Action Space** - The set of all possible actions that an agent can take in a reinforcement learning environment.

**AI-Robot Brain** - The cognitive layer of humanoid robotics where artificial intelligence algorithms process sensory information and generate intelligent behaviors.

## B

**Bounding Box** - A rectangular frame that encloses an object in an image, typically defined by its top-left and bottom-right coordinates or center point with width and height.

## C

**Cognitive Planning** - The process of determining a sequence of actions to achieve a goal, taking into account environmental constraints and robot capabilities.

**Computer Vision** - A field of artificial intelligence that trains computers to interpret and understand the visual world through digital images and videos.

## D

**Deep Learning** - A subset of machine learning that uses neural networks with multiple layers to model and understand complex patterns in data.

**Digital Twin** - A virtual replica of a physical system that allows for simulation, analysis, and optimization before implementing in the real world.

**Domain Randomization** - A technique in simulation where environmental parameters are randomly varied to create robust AI models that can generalize to real-world conditions.

## E

**Embodiment** - The concept that an agent's physical form and interactions with the environment influence its cognition and behavior.

## G

**Gazebo** - A 3D simulation environment that provides realistic physics simulation, high-quality graphics, and convenient programmatic interfaces for robotics.

**GPU Acceleration** - The use of graphics processing units to accelerate computational tasks, particularly useful for deep learning and real-time processing.

## H

**Humanoid Robot** - A robot with a human-like body structure, typically featuring a head, torso, two arms, and two legs.

## I

**Isaac Sim** - NVIDIA's robotics simulation application built on NVIDIA Omniverse, providing high-fidelity simulation and synthetic data generation.

**IMU (Inertial Measurement Unit)** - A sensor that measures specific force, angular rate, and sometimes magnetic fields, used for navigation and motion tracking.

## L

**LiDAR (Light Detection and Ranging)** - A remote sensing method that uses light in the form of a pulsed laser to measure distances and create 3D maps.

**Locomotion** - The ability to move from one place to another, particularly relevant for walking or other forms of movement in humanoid robots.

## M

**Machine Learning** - A branch of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.

**Middleware** - Software that provides common services and capabilities to applications beyond what's offered by the operating system.

## N

**Natural Language Processing (NLP)** - A field of artificial intelligence focused on the interaction between computers and humans through natural language.

**Node** - An executable that uses ROS to communicate with other nodes in a distributed system.

## O

**Omniverse** - NVIDIA's platform for 3D design collaboration and world simulation, used as the foundation for Isaac Sim.

## P

**Path Planning** - The computational problem of finding a valid sequence of configurations to move from a start to a goal position while avoiding obstacles.

**Perception** - The process by which robots interpret sensory information to understand their environment.

**Physics Simulation** - The computational modeling of physical phenomena such as motion, collision, and interaction of objects.

## R

**Reinforcement Learning** - A type of machine learning where an agent learns to make decisions by performing actions and receiving rewards or penalties.

**Robot Operating System (ROS)** - A flexible framework for writing robot software, providing hardware abstraction, device drivers, libraries, and more.

**ROS 2** - The second generation of the Robot Operating System with improved architecture for production systems.

**ROS 2 Humble Hawksbill** - A long-term support (LTS) distribution of ROS 2, released in May 2022.

## S

**Sensor Fusion** - The process of combining data from multiple sensors to improve the accuracy and reliability of information.

**Sim-to-Real Transfer** - The process of transferring learned behaviors or models from simulation to real-world robots.

**SLAM (Simultaneous Localization and Mapping)** - The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

**Synthetic Data** - Artificially generated data that mimics real-world data, often used to train AI models when real data is scarce or expensive to obtain.

## T

**Topic** - A named bus over which nodes exchange messages in ROS, following a publish/subscribe communication pattern.

**Trajectory** - A path that specifies the position, velocity, and acceleration of a robot over time.

## U

**URDF (Unified Robot Description Format)** - An XML format used to describe robot models in ROS, including physical and visual properties.

**Unity** - A cross-platform game engine that can be used for high-fidelity robotics simulation and visualization.

## V

**VSLAM (Visual Simultaneous Localization and Mapping)** - SLAM that uses visual sensors (cameras) as the primary input for localization and mapping.

**Vision-Language-Action (VLA)** - Systems that integrate computer vision, natural language processing, and robotic action execution.

**VLA System** - A system that connects visual perception, language understanding, and physical action to enable natural human-robot interaction.

## W

**Whisper** - OpenAI's automatic speech recognition (ASR) system that converts speech to text with high accuracy.

## X

**Xacro** - An XML macro language for generating URDF files, allowing for more modular and reusable robot descriptions.

## Y

**YAML (YAML Ain't Markup Language)** - A human-readable data serialization format often used for configuration files in ROS.

## Z

**Zero-Shot Learning** - The ability of a model to perform tasks it has never been explicitly trained on, using learned patterns from related tasks.